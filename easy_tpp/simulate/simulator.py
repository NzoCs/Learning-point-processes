from easy_tpp.config_factory import SimulatorConfig
import json
import os
import numpy as np
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm
import random
import torch
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Simulator:
    """
    A wrapper class to generate simulations using a pre-trained temporal point process model.

    This class takes a configuration object, a pre-trained model, and historical data
    (optional but required for simulation continuation) to generate event sequences.
    The generated sequences are then formatted and saved in a structure similar to
    the data generated by `easy_tpp.data_gen`, allowing for unified data handling.
    The data can be split into training, development, and testing sets.
    """
    
    def __init__(self, simulator_config: SimulatorConfig) -> None:
        """
        Initializes the Simulator.

        Args:
            simulator_config (SimulatorConfig): Configuration object containing simulation parameters.
        """
        
        self.save_dir = simulator_config.save_dir
        self.start_time = simulator_config.start_time
        self.end_time = simulator_config.end_time
        self.history_data = simulator_config.history_data
        self.model = simulator_config.pretrained_model
        self.splits = simulator_config.splits
        self.seed = simulator_config.seed
        self.num_simulations = getattr(simulator_config, 'num_simulations', 100) # Default to 100 if not specified

        logger.info(f"Simulator initialized with config: save_dir={self.save_dir}, "
                    f"time_interval=[{self.start_time}, {self.end_time}], "
                    f"num_simulations={self.num_simulations}, seed={self.seed}")

        # --- Robustness Check ---
        if self.model is None:
            raise ValueError("A pre-trained model (pretrained_model) must be provided in the configuration.")
        if not hasattr(self.model, 'simulate') or not callable(self.model.simulate):
            raise AttributeError(f"The provided model of type {type(self.model).__name__} does not have a callable 'simulate' method.")
        if not hasattr(self.model, 'num_event_types'):
             if hasattr(self.model, 'config') and hasattr(self.model.config, 'num_event_types'):
                 self.num_event_types = self.model.config.num_event_types
             else:
                 logger.warning("Could not determine 'num_event_types' from the model. Metadata might be incomplete.")
                 self.num_event_types = None
        else:
            self.num_event_types = self.model.num_event_types

        # --- Seed Setting ---
        if self.seed is not None:
            logger.info(f"Setting random seed to {self.seed}")
            random.seed(self.seed)
            np.random.seed(self.seed)
            torch.manual_seed(self.seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(self.seed)

    def run(self) -> None:
        """
        Executes the simulation process.

        Generates event sequences based on the provided model and history data (if any),
        formats them, splits them according to the configuration, and saves them to disk
        along with metadata. Requires `history_data` to be provided in the config.
        """
        os.makedirs(self.save_dir, exist_ok=True)
        logger.info(f"Ensured output directory exists: {self.save_dir}")
        
        model = self.model
        history_data = self.history_data
        start_time = self.start_time
        end_time = self.end_time
        
        if history_data is None:
            logger.error("history_data is required for simulation but was not provided.")
            raise ValueError("history_data must be provided for simulation based on historical context.")
        
        logger.info(f"Starting generation of {self.num_simulations} simulations...")
        simulations = []
        
        try:
            data_loader = history_data.get_dataloader(split='test')
        except Exception as e:
            logger.error(f"Failed to get dataloader from history_data: {e}")
            raise RuntimeError("Could not obtain dataloader from the provided history_data object.") from e

        sim_count = 0
        with tqdm(total=self.num_simulations, desc="Simulating sequences") as pbar:
            for batch in data_loader:
                if isinstance(batch, dict):
                     batch_values = list(batch.values())
                elif isinstance(batch, (list, tuple)):
                     batch_values = batch
                else:
                     logger.error(f"Unsupported batch type: {type(batch)}")
                     raise TypeError("Batch from data loader is not a dictionary, list, or tuple.")

                try:
                    time_seq, time_delta_seq, event_seq, simul_mask = model.simulate(
                        start_time=start_time,
                        end_time=end_time,
                        batch=batch_values,
                        batch_size=None
                    )
                except Exception as e:
                    logger.error(f"Error during model simulation: {e}")
                    continue

                batch_size = time_seq.size(0)
                for i in range(batch_size):
                    if sim_count >= self.num_simulations:
                        break

                    mask_i = simul_mask[i]
                    if mask_i.any():
                        simulations.append({
                            'time_seq': time_seq[i][mask_i].clone().detach(),
                            'time_delta_seq': time_delta_seq[i][mask_i].clone().detach(),
                            'event_seq': event_seq[i][mask_i].clone().detach(),
                        })
                        sim_count += 1
                        pbar.update(1)

                if sim_count >= self.num_simulations:
                    logger.info(f"Target number of simulations ({self.num_simulations}) reached.")
                    break

        if sim_count < self.num_simulations:
             logger.warning(f"Generated only {sim_count} simulations, less than the requested {self.num_simulations}.")
             self.num_simulations = sim_count

        logger.info(f"Successfully generated {len(simulations)} simulations.")
        
        logger.info("Formatting generated simulations...")
        formatted_data = self.format_multivariate_simulations(
            simulations, self.num_event_types
        )
        logger.info("Formatting complete.")
        
        self.save_data(formatted_data)
    
    def format_multivariate_simulations(self, simulations: List[Dict], dim_process: Optional[int]) -> List[Dict]:
        """
        Formats the raw simulation results into a list of dictionaries, one per sequence.

        Each dictionary follows a structure similar to Hugging Face datasets,
        containing event times, time deltas, event types, sequence length, etc.

        Args:
            simulations (List[Dict]): A list where each dict contains tensors
                                      ('time_seq', 'time_delta_seq', 'event_seq')
                                      for a single simulated sequence.
            dim_process (Optional[int]): The number of event types (dimensionality) in the process.

        Returns:
            List[Dict]: A list of dictionaries, each representing a formatted sequence.
        """
        formatted_data = []
        
        for seq_idx, sim in enumerate(tqdm(simulations, desc="Formatting sequences")):
            times = sim['time_seq']
            events = sim['event_seq']
            time_deltas = sim['time_delta_seq']
            
            times_list = times.cpu().tolist()
            events_list = events.cpu().long().tolist()
            time_deltas_list = time_deltas.cpu().tolist()
            
            if not times_list:
                logger.warning(f"Skipping empty simulation sequence at index {seq_idx}.")
                continue
            
            first_sim_time = times_list[0]
            time_since_start = [t - first_sim_time for t in times_list]

            seq_dict = {
                'dim_process': dim_process if dim_process is not None else -1,
                'seq_len': len(times_list),
                'seq_idx': seq_idx,
                'time_since_start': time_since_start,
                'time_since_last_event': time_deltas_list,
                'type_event': events_list,
                'timestamps': times_list
            }
            formatted_data.append(seq_dict)
        
        return formatted_data
    
    def save_data(self, formatted_data: List[Dict]) -> None:
        """
        Saves the formatted simulation data into train, dev, and test splits based on
        the ratios specified in the configuration. Also saves metadata.

        Args:
            formatted_data (List[Dict]): The list of formatted simulation sequences.
        """
        num_total_seqs = len(formatted_data)
        if num_total_seqs == 0:
            logger.warning("No formatted data to save.")
            self.save_metadata(formatted_data, {})
            return

        if self.seed is not None:
            random.shuffle(formatted_data)
            logger.info("Shuffled formatted data using the provided seed.")
        else:
            random.shuffle(formatted_data)
            logger.info("Shuffled formatted data randomly (no seed provided).")

        train_ratio, dev_ratio, test_ratio = self.splits
        if not np.isclose(train_ratio + dev_ratio + test_ratio, 1.0):
             logger.warning(f"Split ratios {self.splits} do not sum to 1. Normalizing...")
             total = train_ratio + dev_ratio + test_ratio
             train_ratio /= total
             dev_ratio /= total
             test_ratio /= total

        num_train = int(train_ratio * num_total_seqs)
        num_dev = int(dev_ratio * num_total_seqs)
        num_test = num_total_seqs - num_train - num_dev

        split_counts = {'train': num_train, 'dev': num_dev, 'test': num_test}
        logger.info(f"Calculated split sizes: Train={num_train}, Dev={num_dev}, Test={num_test}")

        train_data = formatted_data[:num_train]
        dev_data = formatted_data[num_train : num_train + num_dev]
        test_data = formatted_data[num_train + num_dev :]

        split_filenames = {
            'train': 'train.json',
            'dev': 'dev.json',
            'test': 'test.json'
        }

        for split_name, data_split in [('train', train_data), ('dev', dev_data), ('test', test_data)]:
            if data_split:
                filepath = os.path.join(self.save_dir, split_filenames[split_name])
                self.save_json(data_split, filepath)
                logger.info(f"Saved {split_name} data ({len(data_split)} sequences) to {filepath}")
            else:
                logger.info(f"Skipping save for empty split: {split_name}")

        self.save_metadata(formatted_data, split_counts)
        logger.info(f"All simulated data splits have been saved in {self.save_dir}")
    
    def save_json(self, data: List[Dict], filepath: str) -> None:
        """
        Saves data to a JSON file with indentation.

        Args:
            data (List[Dict]): The data to save (typically a list of dictionaries).
            filepath (str): The full path where the JSON file will be saved.
        """
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
        except IOError as e:
            logger.error(f"Failed to save JSON file to {filepath}: {e}")
        except TypeError as e:
             logger.error(f"Data structure not serializable to JSON for {filepath}: {e}")

    def save_metadata(self, formatted_data: List[Dict], split_counts: Dict[str, int]) -> None:
        """
        Saves metadata about the simulation run, including configuration details,
        total event counts, and data split information.

        Args:
            formatted_data (List[Dict]): The list of all formatted sequences (used for stats).
            split_counts (Dict[str, int]): Dictionary containing the number of sequences
                                           in each split ('train', 'dev', 'test').
        """
        total_events = sum(item.get('seq_len', 0) for item in formatted_data)
        avg_seq_len = total_events / len(formatted_data) if formatted_data else 0

        metadata = {
            'simulation_summary': {
                'total_sequences_generated': len(formatted_data),
                'total_events_generated': total_events,
                'average_sequence_length': round(avg_seq_len, 2),
                'dimension': self.num_event_types if self.num_event_types is not None else 'Unknown',
                'simulation_time_interval': [self.start_time, self.end_time],
                'generating_model': self.model.__class__.__name__ if self.model else 'Unknown',
                'seed_used': self.seed
            },
            'data_split_counts': split_counts,
            'saved_files': {
                 'train': os.path.join(self.save_dir, 'train.json') if split_counts.get('train', 0) > 0 else None,
                 'dev': os.path.join(self.save_dir, 'dev.json') if split_counts.get('dev', 0) > 0 else None,
                 'test': os.path.join(self.save_dir, 'test.json') if split_counts.get('test', 0) > 0 else None,
                 'metadata': os.path.join(self.save_dir, 'metadata.json')
            }
        }
        
        if hasattr(self.model, 'get_model_metadata') and callable(self.model.get_model_metadata):
            try:
                model_metadata = self.model.get_model_metadata()
                if isinstance(model_metadata, dict):
                     metadata['model_specific_parameters'] = model_metadata
                else:
                     logger.warning("Model's get_model_metadata did not return a dictionary. Skipping.")
            except Exception as e:
                logger.warning(f"Could not retrieve or use model-specific metadata: {e}")

        meta_filepath = os.path.join(self.save_dir, 'metadata.json')
        self.save_json(metadata, meta_filepath)
        logger.info(f"Metadata saved to {meta_filepath}")
