GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
INFO:datasets:PyTorch version 2.6.0 available.
INFO:easy_tpp.preprocess.data_loader:Train dataset created with 9000 sequences
INFO:easy_tpp.preprocess.data_loader:Validation dataset created with 3000 sequences
/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/users/regnaguen/Learning-point-processes/main/train/checkpoints/SAHP/self_correcting/trained_models exists and is not empty.
Restoring states from the checkpoint path at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/gpfs/users/regnaguen/Learning-point-processes/main/train/SAHP/self_correcting/happy_koala_488/trained_models' to '/gpfs/users/regnaguen/Learning-point-processes/main/train/checkpoints/SAHP/self_correcting/trained_models', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                   | Type                          | Params | Mode 
---------------------------------------------------------------------------------
0 | layer_type_emb         | Embedding                     | 64     | train
1 | layer_position_emb     | TimeShiftedPositionalEncoding | 16     | train
2 | layer_intensity_hidden | Linear                        | 33     | train
3 | softplus               | ScaledSoftplus                | 1      | train
4 | stack_layers           | ModuleList                    | 6.3 K  | train
5 | mu                     | Sequential                    | 32     | train
6 | eta                    | Sequential                    | 32     | train
7 | gamma                  | Sequential                    | 32     | train
---------------------------------------------------------------------------------
6.5 K     Trainable params
0         Non-trainable params
6.5 K     Total params
0.026     Total estimated model params size (MB)
29        Modules in train mode
0         Modules in eval mode
Restored all states from the checkpoint at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.802
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.802
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.802
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.802
Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.802
`Trainer.fit` stopped: `max_epochs=1000` reached.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Restoring states from the checkpoint path at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Restoring states from the checkpoint path at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at ./checkpoints/SAHP/self_correcting/trained_models/best.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.

Simulating sequences:   0%|          | 0/200 [00:00<?, ?time/s][A
Simulating sequences:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94.60298156738281/200 [00:00<00:00, 5269.43time/s][ATraceback (most recent call last):
  File "/gpfs/users/regnaguen/Learning-point-processes/main/train/train.py", line 35, in <module>
    main()
  File "/gpfs/users/regnaguen/Learning-point-processes/main/train/train.py", line 31, in main
    plrunner.predict()
  File "/gpfs/users/regnaguen/Learning-point-processes/easy_tpp/runner/trainer.py", line 234, in predict
    predictions = trainer.predict(
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 887, in predict
    return call._call_and_handle_interrupt(
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 928, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1051, in _run_stage
    return self.predict_loop.run()
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py", line 125, in run
    self._predict_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py", line 255, in _predict_step
    predictions = call._call_strategy_hook(trainer, "predict_step", *step_args)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 328, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 438, in predict_step
    return self.lightning_module.predict_step(*args, **kwargs)
  File "/gpfs/users/regnaguen/Learning-point-processes/easy_tpp/models/basemodel.py", line 470, in predict_step
    simul_time_seq, simul_time_delta_seq, simul_event_seq, simul_mask = self.simulate(
  File "/gpfs/users/regnaguen/Learning-point-processes/easy_tpp/models/basemodel.py", line 802, in simulate
    marked_last_time = torch.where(
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

                                                                                         [Asrun: error: ruche-gpu18: task 0: Exited with exit code 1
