GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
INFO:datasets:PyTorch version 2.6.0 available.
Restoring states from the checkpoint path at ./checkpoints/AttNHP/self_correcting/trained_models/best.ckpt
Traceback (most recent call last):
  File "/gpfs/users/regnaguen/Learning-point-processes/main/train/train.py", line 35, in <module>
    main()
  File "/gpfs/users/regnaguen/Learning-point-processes/main/train/train.py", line 31, in main
    plrunner.predict()
  File "/gpfs/users/regnaguen/Learning-point-processes/easy_tpp/runner/trainer.py", line 253, in predict
    predictions = trainer.predict(
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 887, in predict
    return call._call_and_handle_interrupt(
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 928, in _predict_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 409, in _restore_modules_and_callbacks
    self.restore_model()
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 286, in restore_model
    self.trainer.strategy.load_model_state_dict(
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 372, in load_model_state_dict
    self.lightning_module.load_state_dict(checkpoint["state_dict"], strict=strict)
  File "/gpfs/workdir/regnaguen/LTPP/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for AttNHP:
	size mismatch for layer_type_emb.weight: copying a param with shape torch.Size([2, 32]) from checkpoint, the shape in current model is torch.Size([2, 16]).
	size mismatch for heads.0.0.self_attn.linears.0.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.0.0.self_attn.linears.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for heads.0.0.self_attn.linears.1.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.0.0.self_attn.linears.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for heads.0.0.self_attn.linears.2.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.0.0.self_attn.linears.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for heads.1.0.self_attn.linears.0.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.1.0.self_attn.linears.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for heads.1.0.self_attn.linears.1.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.1.0.self_attn.linears.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for heads.1.0.self_attn.linears.2.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for heads.1.0.self_attn.linears.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for inten_linear.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 32]).
	size mismatch for layer_event_emb.weight: copying a param with shape torch.Size([32, 48]) from checkpoint, the shape in current model is torch.Size([16, 32]).
	size mismatch for layer_event_emb.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).
	size mismatch for layer_intensity.0.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 32]).
srun: error: ruche-gpu17: task 0: Exited with exit code 1
