{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995eb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ea8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_mark = 2\n",
    "seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6984e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_event_time = torch.zeros(\n",
    "            (batch_size, num_mark), dtype=torch.float32\n",
    "        )\n",
    "last_event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf20f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_seq = torch.randint(\n",
    "            0, 2, (batch_size, seq_len), dtype=torch.float32\n",
    "        )\n",
    "event_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8a3861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4714, 3.1033, 3.2334, 5.0800, 5.5261],\n",
       "        [0.0943, 1.5425, 2.3193, 2.4992, 2.7180],\n",
       "        [0.7317, 0.8511, 1.4375, 1.8325, 3.5591],\n",
       "        [0.1244, 0.5622, 1.8200, 2.2945, 3.5384]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_seq = torch.randn(\n",
    "            (batch_size, seq_len), dtype=torch.float32\n",
    "        ).abs()\n",
    "time_seq = time_seq.cumsum(dim=1)\n",
    "time_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a003ddd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5261, 3.1033],\n",
       "        [2.4992, 2.7180],\n",
       "        [1.8325, 3.5591],\n",
       "        [0.1244, 3.5384]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for mark in range(num_mark):\n",
    "            mark_mask = (event_seq == mark)  # [batch_size, seq_len]\n",
    "            if mark_mask.any():\n",
    "                # Op√©ration vectoris√©e avec masquage efficace\n",
    "                masked_times = time_seq.masked_fill(~mark_mask, float(\"-inf\"))\n",
    "                max_times, _ = masked_times.max(dim=1)\n",
    "                valid_mask = (max_times != float(\"-inf\"))\n",
    "                last_event_time[valid_mask, mark] = max_times[valid_mask]\n",
    "last_event_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcc01f",
   "metadata": {},
   "source": [
    "Le masque bool√©en joue le meme role que index_select en selectionnant seulement certaines colonnes/lignes ... dans une dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cae051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_mask:  tensor([ True, False,  True,  True])\n",
      "type_pred: tensor([0, 0, 1]) shape:  torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.5261, 1.8325, 3.5384])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_mask = (last_event_time[:, 1]  > 3)\n",
    "print(\"valid_mask: \", valid_mask)\n",
    "type_pred = torch.randint(0, 2, (batch_size,))\n",
    "print(\"type_pred:\", type_pred, \"shape: \", type_pred.shape)\n",
    "last_event_time[valid_mask, type_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4f258",
   "metadata": {},
   "source": [
    "# Exemples d'indexation avanc√©e avec PyTorch\n",
    "\n",
    "Ce notebook pr√©sente diff√©rentes techniques d'indexation avanc√©e pour manipuler efficacement les tenseurs PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3684a2",
   "metadata": {},
   "source": [
    "## 1. Indexation de base et slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6031dd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenseur original shape: torch.Size([3, 4, 5])\n",
      "Donn√©es:\n",
      " tensor([[[ 2.1499,  0.6631,  0.1087, -0.4316,  1.3128],\n",
      "         [ 0.6359,  1.3980,  0.8533,  0.3231, -0.8725],\n",
      "         [ 1.3129,  0.8505, -0.2898,  0.9754,  0.8462],\n",
      "         [-1.1346,  1.1589, -1.8925, -0.6487, -1.6561]],\n",
      "\n",
      "        [[ 0.9511, -1.1479,  0.5666, -1.6383, -1.5595],\n",
      "         [ 0.5956, -0.0333,  0.2996, -0.4183,  1.6169],\n",
      "         [ 0.3581, -1.0088, -1.7175,  0.5518,  1.6188],\n",
      "         [ 1.4274,  2.2002,  0.2962,  0.0888, -0.5887]],\n",
      "\n",
      "        [[ 0.6040, -0.0827, -0.2848, -0.1057, -0.6897],\n",
      "         [ 0.0200, -0.1890,  0.3926,  0.3901, -0.4661],\n",
      "         [ 0.6775, -0.2156,  0.9627,  0.0132,  0.1663],\n",
      "         [-1.5046,  1.9544,  1.2686, -1.6510, -1.8305]]])\n",
      "\n",
      "1. Premier √©l√©ment (batch 0): torch.Size([4, 5])\n",
      "2. Derni√®re colonne: torch.Size([3, 4])\n",
      "3. Slice du milieu: torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ons un tenseur 3D pour nos exemples\n",
    "data = torch.randn(3, 4, 5)\n",
    "print(\"Tenseur original shape:\", data.shape)\n",
    "print(\"Donn√©es:\\n\", data)\n",
    "\n",
    "# Indexation basique\n",
    "print(\"\\n1. Premier √©l√©ment (batch 0):\", data[0].shape)\n",
    "print(\"2. Derni√®re colonne:\", data[:, :, -1].shape)\n",
    "print(\"3. Slice du milieu:\", data[:, 1:3, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7a779",
   "metadata": {},
   "source": [
    "## 2. Indexation avanc√©e avec torch.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2913e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tensor:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Indices:\n",
      " tensor([[0, 2],\n",
      "        [1, 3],\n",
      "        [0, 3]])\n",
      "R√©sultat gather:\n",
      " tensor([[ 1,  3],\n",
      "        [ 6,  8],\n",
      "        [ 9, 12]])\n",
      "\n",
      "Exemple avec √©v√©nements:\n",
      "Events:\n",
      " tensor([[0, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 1],\n",
      "        [1, 1, 1, 0, 1]])\n",
      "Times:\n",
      " tensor([[0.0917, 1.1058, 2.4856, 3.1408, 4.7153],\n",
      "        [0.3074, 0.8413, 2.3905, 2.6149, 3.0617],\n",
      "        [1.5503, 1.8348, 3.1659, 4.4744, 6.1252]])\n",
      "Derniers indices pour type 1:\n",
      " tensor([[3],\n",
      "        [4],\n",
      "        [4]])\n",
      "Derniers temps pour type 1:\n",
      " tensor([[3.1408],\n",
      "        [3.0617],\n",
      "        [6.1252]])\n"
     ]
    }
   ],
   "source": [
    "# torch.gather permet de s√©lectionner des √©l√©ments selon des indices\n",
    "source = torch.tensor([[1, 2, 3, 4], \n",
    "                       [5, 6, 7, 8], \n",
    "                       [9, 10, 11, 12]])\n",
    "\n",
    "# Indices √† s√©lectionner pour chaque ligne\n",
    "indices = torch.tensor([[0, 2], [1, 3], [0, 3]])\n",
    "\n",
    "print(\"Source tensor:\\n\", source)\n",
    "print(\"Indices:\\n\", indices)\n",
    "\n",
    "# Gather sur la dimension 1 (colonnes)\n",
    "gathered = torch.gather(source, dim=1, index=indices)\n",
    "print(\"R√©sultat gather:\\n\", gathered)\n",
    "\n",
    "# Exemple avec nos donn√©es de simulation : s√©lectionner le dernier √©v√©nement de chaque type\n",
    "batch_size, seq_len = 3, 5\n",
    "events = torch.randint(0, 2, (batch_size, seq_len))\n",
    "times = torch.randn(batch_size, seq_len).abs().cumsum(dim=1)\n",
    "\n",
    "print(\"\\nExemple avec √©v√©nements:\")\n",
    "print(\"Events:\\n\", events)\n",
    "print(\"Times:\\n\", times)\n",
    "\n",
    "# Trouver l'index du dernier √©v√©nement de type 1 pour chaque batch\n",
    "last_indices = []\n",
    "for b in range(batch_size):\n",
    "    mask = events[b] == 1\n",
    "    if mask.any():\n",
    "        last_idx = torch.where(mask)[0][-1]\n",
    "    else:\n",
    "        last_idx = torch.tensor(0)  # fallback\n",
    "    last_indices.append(last_idx)\n",
    "\n",
    "last_indices = torch.stack(last_indices).unsqueeze(1)\n",
    "print(\"Derniers indices pour type 1:\\n\", last_indices)\n",
    "\n",
    "# Utiliser gather pour r√©cup√©rer les temps correspondants\n",
    "last_times = torch.gather(times, dim=1, index=last_indices)\n",
    "print(\"Derniers temps pour type 1:\\n\", last_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81946dee",
   "metadata": {},
   "source": [
    "## 3. Indexation avec des masques bool√©ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfda8128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs:\n",
      " tensor([[ 1.4011,  1.5079, -1.0909,  0.4197,  0.3895,  0.5238],\n",
      "        [-2.1130,  0.4523, -0.9354, -1.7169, -0.6213, -0.8477],\n",
      "        [-0.8427,  0.7809,  0.7720, -1.2123,  1.2467,  0.7433],\n",
      "        [-0.5526,  2.0318, -0.9890, -2.5797,  0.3550, -0.7316]])\n",
      "\n",
      "Masque positif:\n",
      " tensor([[ True,  True, False,  True,  True,  True],\n",
      "        [False,  True, False, False, False, False],\n",
      "        [False,  True,  True, False,  True,  True],\n",
      "        [False,  True, False, False,  True, False]])\n",
      "Valeurs positives: tensor([1.4011, 1.5079, 0.4197, 0.3895, 0.5238, 0.4523, 0.7809, 0.7720, 1.2467,\n",
      "        0.7433]) ...\n",
      "\n",
      "Valeurs avec n√©gatifs remplac√©s par 0:\n",
      " tensor([[1.4011, 1.5079, 0.0000, 0.4197, 0.3895, 0.5238],\n",
      "        [0.0000, 0.4523, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7809, 0.7720, 0.0000, 1.2467, 0.7433],\n",
      "        [0.0000, 2.0318, 0.0000, 0.0000, 0.3550, 0.0000]])\n",
      "\n",
      "Valeurs entre 0 et 1: tensor([0.4197, 0.3895, 0.5238, 0.4523, 0.7809, 0.7720, 0.7433, 0.3550])\n",
      "\n",
      "Masque de padding (True = padding):\n",
      "tensor([[False, False, False,  True,  True],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True]])\n",
      "Donn√©es avec padding masqu√©:\n",
      " tensor([[-0.4380, -0.8599, -0.0952,    -inf,    -inf],\n",
      "        [-1.2057, -1.5206,  0.1903, -0.0626, -0.8769],\n",
      "        [ 1.5990, -1.6312,    -inf,    -inf,    -inf],\n",
      "        [ 0.6191, -0.0372,  1.6390, -0.0213,    -inf]])\n"
     ]
    }
   ],
   "source": [
    "# Masques bool√©ens pour filtrage et s√©lection conditionnelle\n",
    "values = torch.randn(4, 6)\n",
    "print(\"Valeurs:\\n\", values)\n",
    "\n",
    "# Masque pour valeurs positives\n",
    "positive_mask = values > 0\n",
    "print(\"\\nMasque positif:\\n\", positive_mask)\n",
    "\n",
    "# S√©lectionner seulement les valeurs positives (retourne un tenseur 1D)\n",
    "positive_values = values[positive_mask]\n",
    "print(\"Valeurs positives:\", positive_values[:10], \"...\")  # Affichage partiel\n",
    "\n",
    "# Utilisation de masked_fill pour remplacer des valeurs\n",
    "values_filled = values.masked_fill(values < 0, 0.0)\n",
    "print(\"\\nValeurs avec n√©gatifs remplac√©s par 0:\\n\", values_filled)\n",
    "\n",
    "# Utilisation de masked_select pour un filtrage plus complexe\n",
    "mask_complex = (values > 0) & (values < 1)\n",
    "selected = values.masked_select(mask_complex)\n",
    "print(\"\\nValeurs entre 0 et 1:\", selected)\n",
    "\n",
    "# Exemple pratique : masquer les paddings dans une s√©quence\n",
    "seq_len = torch.tensor([3, 5, 2, 4])  # Longueurs r√©elles de chaque s√©quence\n",
    "max_len = 5\n",
    "batch_size = seq_len.size(0)\n",
    "\n",
    "# Cr√©er un masque de padding\n",
    "padding_mask = torch.arange(max_len).unsqueeze(0) >= seq_len.unsqueeze(1)\n",
    "print(f\"\\nMasque de padding (True = padding):\\n{padding_mask}\")\n",
    "\n",
    "# Appliquer le masque sur des donn√©es\n",
    "sequence_data = torch.randn(batch_size, max_len)\n",
    "masked_data = sequence_data.masked_fill(padding_mask, float('-inf'))\n",
    "print(\"Donn√©es avec padding masqu√©:\\n\", masked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0592c",
   "metadata": {},
   "source": [
    "## 4. Indexation avec torch.index_select et torch.take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44820990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice originale:\n",
      " tensor([[ 1.4924, -1.5938, -0.9453,  1.2735,  0.4558,  0.8859],\n",
      "        [-0.3927,  0.4843,  0.2566, -0.1102, -1.2561,  0.9141],\n",
      "        [ 0.6275,  0.5636,  1.0326, -0.5624,  0.0315,  1.9188],\n",
      "        [-0.3732, -0.0578,  0.3002,  2.2216, -0.0849, -0.6441]])\n",
      "\n",
      "Lignes s√©lectionn√©es [0, 2, 3]:\n",
      " tensor([[ 1.4924, -1.5938, -0.9453,  1.2735,  0.4558,  0.8859],\n",
      "        [ 0.6275,  0.5636,  1.0326, -0.5624,  0.0315,  1.9188],\n",
      "        [-0.3732, -0.0578,  0.3002,  2.2216, -0.0849, -0.6441]])\n",
      "\n",
      "Colonnes s√©lectionn√©es [1, 3, 5]:\n",
      " tensor([[-1.5938,  1.2735,  0.8859],\n",
      "        [ 0.4843, -0.1102,  0.9141],\n",
      "        [ 0.5636, -0.5624,  1.9188],\n",
      "        [-0.0578,  2.2216, -0.6441]])\n",
      "\n",
      "Tenseur plat:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "√âl√©ments pris aux positions [0, 5, 7, 11]: tensor([ 0,  5,  7, 11])\n",
      "\n",
      "Embeddings s√©lectionn√©s shape: torch.Size([2, 4, 128])\n",
      "Premier embedding du premier batch: tensor([ 1.5345, -0.7401,  0.4479,  0.8496,  0.0639])\n"
     ]
    }
   ],
   "source": [
    "# torch.index_select : s√©lectionner selon des indices sur une dimension\n",
    "matrix = torch.randn(4, 6)\n",
    "print(\"Matrice originale:\\n\", matrix)\n",
    "\n",
    "# S√©lectionner des lignes sp√©cifiques\n",
    "row_indices = torch.tensor([0, 2, 3])\n",
    "selected_rows = torch.index_select(matrix, dim=0, index=row_indices)\n",
    "print(\"\\nLignes s√©lectionn√©es [0, 2, 3]:\\n\", selected_rows)\n",
    "\n",
    "# S√©lectionner des colonnes sp√©cifiques\n",
    "col_indices = torch.tensor([1, 3, 5])\n",
    "selected_cols = torch.index_select(matrix, dim=1, index=col_indices)\n",
    "print(\"\\nColonnes s√©lectionn√©es [1, 3, 5]:\\n\", selected_cols)\n",
    "\n",
    "# torch.take : traite le tenseur comme un tableau plat\n",
    "flat_tensor = torch.arange(12).reshape(3, 4)\n",
    "print(\"\\nTenseur plat:\\n\", flat_tensor)\n",
    "\n",
    "# Indices en tant que positions absolues\n",
    "indices = torch.tensor([0, 5, 7, 11])\n",
    "taken = torch.take(flat_tensor, indices)\n",
    "print(\"√âl√©ments pris aux positions [0, 5, 7, 11]:\", taken)\n",
    "\n",
    "# Exemple pratique : s√©lectionner des embeddings\n",
    "vocab_size, embedding_dim = 1000, 128\n",
    "embeddings = torch.randn(vocab_size, embedding_dim)\n",
    "\n",
    "# S√©quences d'indices de mots\n",
    "word_indices = torch.tensor([[1, 5, 23, 7], [45, 2, 8, 12]])\n",
    "batch_size, seq_length = word_indices.shape\n",
    "\n",
    "# S√©lectionner les embeddings correspondants\n",
    "selected_embeddings = torch.index_select(embeddings, dim=0, \n",
    "                                        index=word_indices.flatten())\n",
    "selected_embeddings = selected_embeddings.view(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "print(f\"\\nEmbeddings s√©lectionn√©s shape: {selected_embeddings.shape}\")\n",
    "print(\"Premier embedding du premier batch:\", selected_embeddings[0, 0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed3330",
   "metadata": {},
   "source": [
    "## 5. Indexation multi-dimensionnelle avanc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1207acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenseur 3D shape: torch.Size([2, 3, 4])\n",
      "Donn√©es:\n",
      " tensor([[[ 1.2225,  0.0716,  0.3245, -0.3487],\n",
      "         [-1.2330,  0.8190,  0.9548,  1.0756],\n",
      "         [ 0.3341,  0.4133, -0.0394,  0.3766]],\n",
      "\n",
      "        [[ 0.6936, -0.5983, -0.5106,  0.8924],\n",
      "         [ 0.6499,  0.7171, -0.9614,  0.5521],\n",
      "         [-0.2948,  0.6610, -0.2536, -0.1455]]])\n",
      "\n",
      "√âl√©ments s√©lectionn√©s: tensor([-1.2330,  0.6610,  0.3245,  0.5521])\n",
      "Range batch: tensor([0, 1])\n",
      "\n",
      "Max values shape: torch.Size([2, 3])\n",
      "Max values:\n",
      " tensor([[1.2225, 1.0756, 0.4133],\n",
      "        [0.8924, 0.7171, 0.6610]])\n",
      "Max indices:\n",
      " tensor([[0, 3, 1],\n",
      "        [3, 1, 1]])\n",
      "\n",
      "Valeurs r√©cup√©r√©es (v√©rification):\n",
      " tensor([[1.2225, 1.0756, 0.4133],\n",
      "        [0.8924, 0.7171, 0.6610]])\n",
      "√âgalit√©: True\n"
     ]
    }
   ],
   "source": [
    "# Indexation avec plusieurs dimensions simultan√©ment\n",
    "batch_size, num_classes, seq_len = 2, 3, 4\n",
    "tensor_3d = torch.randn(batch_size, num_classes, seq_len)\n",
    "print(\"Tenseur 3D shape:\", tensor_3d.shape)\n",
    "print(\"Donn√©es:\\n\", tensor_3d)\n",
    "\n",
    "# Indexation fancy avec des listes de tenseurs\n",
    "batch_idx = torch.tensor([0, 1, 0, 1])  # indices de batch\n",
    "class_idx = torch.tensor([1, 2, 0, 1])  # indices de classe\n",
    "seq_idx = torch.tensor([0, 1, 2, 3])    # indices de s√©quence\n",
    "\n",
    "# S√©lection d'√©l√©ments sp√©cifiques\n",
    "selected_elements = tensor_3d[batch_idx, class_idx, seq_idx]\n",
    "print(\"\\n√âl√©ments s√©lectionn√©s:\", selected_elements)\n",
    "\n",
    "# Utilisation d'arange pour l'indexation\n",
    "batch_range = torch.arange(batch_size)\n",
    "print(f\"Range batch: {batch_range}\")\n",
    "\n",
    "# S√©lectionner le maximum de chaque batch dans chaque classe\n",
    "max_values, max_indices = tensor_3d.max(dim=2)  # max sur seq_len\n",
    "print(f\"\\nMax values shape: {max_values.shape}\")\n",
    "print(\"Max values:\\n\", max_values)\n",
    "print(\"Max indices:\\n\", max_indices)\n",
    "\n",
    "# Utiliser les indices pour r√©cup√©rer les valeurs originales\n",
    "batch_expanded = batch_range.unsqueeze(1).expand(-1, num_classes)\n",
    "class_expanded = torch.arange(num_classes).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "recovered_values = tensor_3d[batch_expanded, class_expanded, max_indices]\n",
    "print(\"\\nValeurs r√©cup√©r√©es (v√©rification):\\n\", recovered_values)\n",
    "print(\"√âgalit√©:\", torch.allclose(max_values, recovered_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd714a5a",
   "metadata": {},
   "source": [
    "## 6. Exemples appliqu√©s aux processus ponctuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b2ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types d'√©v√©nements:\n",
      " tensor([[1, 3, 1, 2, 2, 3, 3, 3],\n",
      "        [3, 1, 3, 1, 2, 0, 3, 0],\n",
      "        [2, 1, 3, 2, 1, 2, 0, 1]])\n",
      "Temps d'√©v√©nements:\n",
      " tensor([[1.0120, 1.7871, 3.0567, 4.7263, 5.7100, 6.1736, 6.5166, 6.6035],\n",
      "        [0.2779, 0.9679, 4.2464, 4.7372, 4.9811, 7.8192, 8.9795, 9.7267],\n",
      "        [0.2971, 1.2268, 2.4551, 2.5715, 4.0258, 4.2429, 5.2187, 5.3796]])\n",
      "\n",
      "√âv√©nements valides (avec masque):\n",
      " tensor([[1, 3, 1, 2, 2, 0, 0, 0],\n",
      "        [3, 1, 3, 1, 2, 0, 3, 0],\n",
      "        [2, 1, 3, 2, 1, 2, 0, 0]])\n",
      "Temps valides:\n",
      " tensor([[1.0120, 1.7871, 3.0567, 4.7263, 5.7100, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2779, 0.9679, 4.2464, 4.7372, 4.9811, 7.8192, 8.9795, 0.0000],\n",
      "        [0.2971, 1.2268, 2.4551, 2.5715, 4.0258, 4.2429, 0.0000, 0.0000]])\n",
      "\n",
      "Derniers temps par type d'√©v√©nement:\n",
      " tensor([[0.0000, 3.0567, 5.7100, 1.7871],\n",
      "        [7.8192, 4.7372, 4.9811, 8.9795],\n",
      "        [0.0000, 4.0258, 4.2429, 2.4551]])\n",
      "\n",
      "Version vectoris√©e:\n",
      " tensor([[0.0000, 3.0567, 5.7100, 1.7871],\n",
      "        [7.8192, 4.7372, 4.9811, 8.9795],\n",
      "        [0.0000, 4.0258, 4.2429, 2.4551]])\n",
      "√âgalit√© avec version naive: True\n"
     ]
    }
   ],
   "source": [
    "# Applications pratiques pour les processus ponctuels\n",
    "batch_size, max_seq_len, num_event_types = 3, 8, 4\n",
    "\n",
    "# Simulation d'√©v√©nements avec longueurs variables\n",
    "seq_lengths = torch.tensor([5, 7, 6])\n",
    "event_types = torch.randint(0, num_event_types, (batch_size, max_seq_len))\n",
    "event_times = torch.randn(batch_size, max_seq_len).abs().cumsum(dim=1)\n",
    "\n",
    "print(\"Types d'√©v√©nements:\\n\", event_types)\n",
    "print(\"Temps d'√©v√©nements:\\n\", event_times)\n",
    "\n",
    "# 1. Masquer les √©v√©nements de padding\n",
    "padding_mask = torch.arange(max_seq_len).unsqueeze(0) < seq_lengths.unsqueeze(1)\n",
    "valid_events = event_types * padding_mask.long()  # 0 pour les paddings\n",
    "valid_times = event_times.masked_fill(~padding_mask, 0.0)\n",
    "\n",
    "print(\"\\n√âv√©nements valides (avec masque):\\n\", valid_events)\n",
    "print(\"Temps valides:\\n\", valid_times)\n",
    "\n",
    "# 2. Trouver le dernier √©v√©nement de chaque type pour chaque s√©quence\n",
    "last_event_times = torch.zeros(batch_size, num_event_types)\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "    for event_type in range(num_event_types):\n",
    "        # Masque pour ce type d'√©v√©nement dans cette s√©quence\n",
    "        type_mask = (event_types[batch_idx] == event_type) & padding_mask[batch_idx]\n",
    "        \n",
    "        if type_mask.any():\n",
    "            # Trouver l'index du dernier √©v√©nement de ce type\n",
    "            last_idx = torch.where(type_mask)[0][-1]\n",
    "            last_event_times[batch_idx, event_type] = event_times[batch_idx, last_idx]\n",
    "\n",
    "print(\"\\nDerniers temps par type d'√©v√©nement:\\n\", last_event_times)\n",
    "\n",
    "# 3. Version vectoris√©e plus efficace\n",
    "def get_last_event_times_vectorized(event_types, event_times, padding_mask, num_event_types):\n",
    "    batch_size, seq_len = event_types.shape\n",
    "    last_times = torch.zeros(batch_size, num_event_types)\n",
    "    \n",
    "    for event_type in range(num_event_types):\n",
    "        # Masque pour ce type d'√©v√©nement\n",
    "        type_mask = (event_types == event_type) & padding_mask\n",
    "        \n",
    "        # Cr√©er un tenseur avec -inf o√π il n'y a pas d'√©v√©nements de ce type\n",
    "        masked_times = event_times.masked_fill(~type_mask, float('-inf'))\n",
    "        \n",
    "        # Prendre le maximum (dernier temps) par batch\n",
    "        max_times, _ = masked_times.max(dim=1)\n",
    "        \n",
    "        # Remplacer -inf par 0 si aucun √©v√©nement de ce type\n",
    "        valid_mask = max_times != float('-inf')\n",
    "        last_times[valid_mask, event_type] = max_times[valid_mask]\n",
    "    \n",
    "    return last_times\n",
    "\n",
    "last_times_vec = get_last_event_times_vectorized(event_types, event_times, padding_mask, num_event_types)\n",
    "print(\"\\nVersion vectoris√©e:\\n\", last_times_vec)\n",
    "print(\"√âgalit√© avec version naive:\", torch.allclose(last_event_times, last_times_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f29dc",
   "metadata": {},
   "source": [
    "## 7. Techniques d'indexation pour l'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38551b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps vectoris√©: 0.0014s\n",
      "Shape r√©sultat: torch.Size([100, 100])\n",
      "\n",
      "Valeurs √† grouper par bins: tensor([ 0.9677,  0.8307, -0.6004, -0.0969, -0.9670])\n",
      "Indices de bins: tensor([2, 3, 2, 9, 3])\n",
      "Sommes par bin:\n",
      " tensor([[ 2.1389e+00,  0.0000e+00,  4.1902e+00,  1.9977e-03,  1.0681e+00,\n",
      "          2.3102e-01, -7.2568e-01,  5.6631e-01,  0.0000e+00, -1.8103e-01],\n",
      "        [ 0.0000e+00,  2.7782e+00, -1.4263e+00,  0.0000e+00,  6.3193e-01,\n",
      "          3.1381e+00, -1.1786e-01,  4.7861e-01,  4.2729e-01,  2.0775e+00],\n",
      "        [-2.1165e+00,  1.6351e+00,  4.4144e-01, -5.4236e-01,  5.4693e-01,\n",
      "          0.0000e+00,  4.1478e-01, -1.1112e-01,  3.2264e+00, -3.7155e-01],\n",
      "        [ 1.4515e+00, -1.0545e+00,  8.9650e-01,  2.7579e+00, -1.7292e-01,\n",
      "         -8.0008e-01,  1.1697e-01, -1.7198e+00, -1.1725e+00, -1.5216e-01]])\n",
      "Tenseur original shape: torch.Size([1000, 1000])\n",
      "Reshaped shape: torch.Size([100, 10, 100, 10])\n",
      "M√™me m√©moire: True\n",
      "Final shape: torch.Size([100, 100, 100])\n",
      "\n",
      "Top-10 values shape: torch.Size([5, 10])\n",
      "Top-10 indices shape: torch.Size([5, 10])\n",
      "Top features shape: torch.Size([5, 10, 64])\n",
      "\n",
      "=== Conseils d'optimisation ===\n",
      "1. Utilisez view() au lieu de reshape() quand possible\n",
      "2. √âvitez les boucles Python, pr√©f√©rez les op√©rations vectoris√©es\n",
      "3. Utilisez scatter/gather pour les op√©rations group√©es\n",
      "4. masked_fill est plus rapide que l'indexation conditionnelle\n",
      "5. topk est optimis√© pour la s√©lection des meilleurs √©l√©ments\n"
     ]
    }
   ],
   "source": [
    "# Techniques avanc√©es pour optimiser les performances\n",
    "\n",
    "# 1. √âviter les boucles Python avec broadcasting\n",
    "def naive_distance_matrix(points):\n",
    "    \"\"\"Version naive avec boucles (LENT)\"\"\"\n",
    "    n = points.shape[0]\n",
    "    distances = torch.zeros(n, n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i, j] = torch.norm(points[i] - points[j])\n",
    "    return distances\n",
    "\n",
    "def vectorized_distance_matrix(points):\n",
    "    \"\"\"Version vectoris√©e (RAPIDE)\"\"\"\n",
    "    # points shape: [n, d]\n",
    "    # Utilisation du broadcasting\n",
    "    diff = points.unsqueeze(1) - points.unsqueeze(0)  # [n, n, d]\n",
    "    distances = torch.norm(diff, dim=2)  # [n, n]\n",
    "    return distances\n",
    "\n",
    "# Test avec des points al√©atoires\n",
    "points = torch.randn(100, 3)\n",
    "\n",
    "# Mesure du temps\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "dist_vectorized = vectorized_distance_matrix(points)\n",
    "time_vectorized = time.time() - start\n",
    "\n",
    "print(f\"Temps vectoris√©: {time_vectorized:.4f}s\")\n",
    "print(f\"Shape r√©sultat: {dist_vectorized.shape}\")\n",
    "\n",
    "# 2. Indexation efficace avec scatter operations\n",
    "batch_size, num_bins = 4, 10\n",
    "values = torch.randn(batch_size, 20)  # 20 valeurs par batch\n",
    "bin_indices = torch.randint(0, num_bins, (batch_size, 20))\n",
    "\n",
    "print(\"\\nValeurs √† grouper par bins:\", values[0, :5])\n",
    "print(\"Indices de bins:\", bin_indices[0, :5])\n",
    "\n",
    "# Sommer les valeurs par bin avec scatter_add\n",
    "bin_sums = torch.zeros(batch_size, num_bins)\n",
    "bin_sums.scatter_add_(1, bin_indices, values)\n",
    "\n",
    "print(\"Sommes par bin:\\n\", bin_sums)\n",
    "\n",
    "# 3. Optimisation m√©moire avec views et squeeze/unsqueeze\n",
    "large_tensor = torch.randn(1000, 1000)\n",
    "print(f\"Tenseur original shape: {large_tensor.shape}\")\n",
    "\n",
    "# View pour reshaper sans copier\n",
    "reshaped = large_tensor.view(100, 10, 100, 10)\n",
    "print(f\"Reshaped shape: {reshaped.shape}\")\n",
    "print(f\"M√™me m√©moire: {reshaped.data_ptr() == large_tensor.data_ptr()}\")\n",
    "\n",
    "# Permute + contiguous pour r√©organiser efficacement\n",
    "permuted = reshaped.permute(0, 2, 1, 3)  # Change l'ordre des dimensions\n",
    "flattened = permuted.contiguous().view(100, 100, 100)  # N√©cessite contiguous()\n",
    "print(f\"Final shape: {flattened.shape}\")\n",
    "\n",
    "# 4. Indexation avec topk pour s√©lection efficace\n",
    "scores = torch.randn(5, 1000)  # 5 samples, 1000 features each\n",
    "k = 10\n",
    "\n",
    "# S√©lectionner les top-k scores pour chaque sample\n",
    "top_values, top_indices = torch.topk(scores, k, dim=1)\n",
    "print(f\"\\nTop-{k} values shape: {top_values.shape}\")\n",
    "print(f\"Top-{k} indices shape: {top_indices.shape}\")\n",
    "\n",
    "# Utiliser les indices pour r√©cup√©rer d'autres informations\n",
    "features = torch.randn(5, 1000, 64)  # Features associ√©es\n",
    "top_features = torch.gather(features, 1, \n",
    "                           top_indices.unsqueeze(-1).expand(-1, -1, 64))\n",
    "print(f\"Top features shape: {top_features.shape}\")\n",
    "\n",
    "print(\"\\n=== Conseils d'optimisation ===\")\n",
    "print(\"1. Utilisez view() au lieu de reshape() quand possible\")\n",
    "print(\"2. √âvitez les boucles Python, pr√©f√©rez les op√©rations vectoris√©es\")\n",
    "print(\"3. Utilisez scatter/gather pour les op√©rations group√©es\")\n",
    "print(\"4. masked_fill est plus rapide que l'indexation conditionnelle\")\n",
    "print(\"5. topk est optimis√© pour la s√©lection des meilleurs √©l√©ments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084bd29",
   "metadata": {},
   "source": [
    "## 8. Diff√©rences entre gather, index_select et take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a42e4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenseur de r√©f√©rence (4x5):\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [40, 41, 42, 43, 44]])\n",
      "Shape: torch.Size([4, 5])\n",
      "\n",
      "============================================================\n",
      "1. TORCH.GATHER - Indexation flexible par dimension\n",
      "============================================================\n",
      "Indices pour gather:\n",
      "tensor([[1, 3, 0, 4],\n",
      "        [4, 2, 1, 0],\n",
      "        [2, 2, 2, 2],\n",
      "        [0, 1, 2, 3]])\n",
      "R√©sultat gather (dim=1):\n",
      "tensor([[11, 13, 10, 14],\n",
      "        [24, 22, 21, 20],\n",
      "        [32, 32, 32, 32],\n",
      "        [40, 41, 42, 43]])\n",
      "Shape: torch.Size([4, 4])\n",
      "\n",
      "Caract√©ristiques de gather:\n",
      "- Pr√©serve le nombre de dimensions\n",
      "- Les indices peuvent √™tre diff√©rents pour chaque 'ligne' (batch)\n",
      "- Shape du r√©sultat = shape des indices\n",
      "- Tr√®s flexible pour l'indexation par batch\n"
     ]
    }
   ],
   "source": [
    "# Comparaison d√©taill√©e entre gather, index_select et take\n",
    "import torch\n",
    "\n",
    "# Cr√©ons un tenseur de r√©f√©rence pour tous les exemples\n",
    "data = torch.tensor([\n",
    "    [10, 11, 12, 13, 14],\n",
    "    [20, 21, 22, 23, 24],\n",
    "    [30, 31, 32, 33, 34],\n",
    "    [40, 41, 42, 43, 44]\n",
    "])\n",
    "print(\"Tenseur de r√©f√©rence (4x5):\")\n",
    "print(data)\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. TORCH.GATHER - Indexation flexible par dimension\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# gather: s√©lectionne des √©l√©ments selon des indices, en pr√©servant la structure\n",
    "indices_gather = torch.tensor([\n",
    "    [1, 3, 0, 4],  # Pour la ligne 0: colonnes 1,3,0,4\n",
    "    [4, 2, 1, 0],  # Pour la ligne 1: colonnes 4,2,1,0  \n",
    "    [2, 2, 2, 2],  # Pour la ligne 2: colonne 2 r√©p√©t√©e\n",
    "    [0, 1, 2, 3]   # Pour la ligne 3: colonnes 0,1,2,3\n",
    "])\n",
    "\n",
    "print(\"Indices pour gather:\")\n",
    "print(indices_gather)\n",
    "\n",
    "gathered = torch.gather(data, dim=1, index=indices_gather)\n",
    "print(\"R√©sultat gather (dim=1):\")\n",
    "print(gathered)\n",
    "print(\"Shape:\", gathered.shape)\n",
    "\n",
    "print(\"\\nCaract√©ristiques de gather:\")\n",
    "print(\"- Pr√©serve le nombre de dimensions\")\n",
    "print(\"- Les indices peuvent √™tre diff√©rents pour chaque 'ligne' (batch)\")\n",
    "print(\"- Shape du r√©sultat = shape des indices\")\n",
    "print(\"- Tr√®s flexible pour l'indexation par batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e661d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. TORCH.INDEX_SELECT - S√©lection de tranches enti√®res\n",
      "============================================================\n",
      "S√©lection de lignes [0, 2, 3]:\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [40, 41, 42, 43, 44]])\n",
      "Shape: torch.Size([3, 5])\n",
      "\n",
      "S√©lection de colonnes [1, 4, 2]:\n",
      "tensor([[11, 14, 12],\n",
      "        [21, 24, 22],\n",
      "        [31, 34, 32],\n",
      "        [41, 44, 42]])\n",
      "Shape: torch.Size([4, 3])\n",
      "\n",
      "Caract√©ristiques d'index_select:\n",
      "- S√©lectionne des tranches COMPL√àTES selon une dimension\n",
      "- Les indices s'appliquent √† TOUTES les lignes/colonnes uniform√©ment\n",
      "- Plus simple que gather mais moins flexible\n",
      "- √âquivalent au slicing avanc√©: data[[0,2,3], :] ou data[:, [1,4,2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. TORCH.INDEX_SELECT - S√©lection de tranches enti√®res\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# index_select: s√©lectionne des lignes/colonnes ENTI√àRES selon des indices\n",
    "indices_rows = torch.tensor([0, 2, 3])  # S√©lectionner lignes 0, 2, 3\n",
    "selected_rows = torch.index_select(data, dim=0, index=indices_rows)\n",
    "print(\"S√©lection de lignes [0, 2, 3]:\")\n",
    "print(selected_rows)\n",
    "print(\"Shape:\", selected_rows.shape)\n",
    "\n",
    "indices_cols = torch.tensor([1, 4, 2])  # S√©lectionner colonnes 1, 4, 2  \n",
    "selected_cols = torch.index_select(data, dim=1, index=indices_cols)\n",
    "print(\"\\nS√©lection de colonnes [1, 4, 2]:\")\n",
    "print(selected_cols)\n",
    "print(\"Shape:\", selected_cols.shape)\n",
    "\n",
    "print(\"\\nCaract√©ristiques d'index_select:\")\n",
    "print(\"- S√©lectionne des tranches COMPL√àTES selon une dimension\")\n",
    "print(\"- Les indices s'appliquent √† TOUTES les lignes/colonnes uniform√©ment\")\n",
    "print(\"- Plus simple que gather mais moins flexible\")\n",
    "print(\"- √âquivalent au slicing avanc√©: data[[0,2,3], :] ou data[:, [1,4,2]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "767dabf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. TORCH.TAKE - Indexation en tableau plat\n",
      "============================================================\n",
      "Tenseur aplati conceptuellement:\n",
      "Position:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Valeur:    [10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 30, 31, 32, 33, 34, 40, 41, 42, 43, 44]\n",
      "\n",
      "Indices take: [0, 5, 7, 12, 19]\n",
      "R√©sultat take: tensor([10, 20, 22, 32, 44])\n",
      "Shape: torch.Size([5])\n",
      "\n",
      "V√©rification manuelle:\n",
      "Index 0 -> position [0, 0] -> valeur 10\n",
      "Index 5 -> position [1, 0] -> valeur 20\n",
      "Index 7 -> position [1, 2] -> valeur 22\n",
      "Index 12 -> position [2, 2] -> valeur 32\n",
      "Index 19 -> position [3, 4] -> valeur 44\n",
      "\n",
      "Caract√©ristiques de take:\n",
      "- Traite le tenseur comme un tableau 1D (ordre row-major)\n",
      "- Indices en positions absolutes\n",
      "- R√©sultat toujours 1D\n",
      "- Utile pour indexation sparse ou √©chantillonnage al√©atoire\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. TORCH.TAKE - Indexation en tableau plat\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# take: traite le tenseur comme un vecteur plat (1D)\n",
    "print(\"Tenseur aplati conceptuellement:\")\n",
    "flat_view = data.flatten()\n",
    "print(\"Position: \", list(range(len(flat_view))))\n",
    "print(\"Valeur:   \", flat_view.tolist())\n",
    "\n",
    "indices_take = torch.tensor([0, 5, 7, 12, 19])  # Positions absolues dans le tableau plat\n",
    "taken = torch.take(data, indices_take)\n",
    "print(f\"\\nIndices take: {indices_take.tolist()}\")\n",
    "print(\"R√©sultat take:\", taken)\n",
    "print(\"Shape:\", taken.shape)\n",
    "\n",
    "print(\"\\nV√©rification manuelle:\")\n",
    "for i, idx in enumerate(indices_take):\n",
    "    row = idx // 5  # Division enti√®re pour trouver la ligne\n",
    "    col = idx % 5   # Modulo pour trouver la colonne\n",
    "    print(f\"Index {idx} -> position [{row}, {col}] -> valeur {data[row, col]}\")\n",
    "\n",
    "print(\"\\nCaract√©ristiques de take:\")\n",
    "print(\"- Traite le tenseur comme un tableau 1D (ordre row-major)\")\n",
    "print(\"- Indices en positions absolutes\")\n",
    "print(\"- R√©sultat toujours 1D\")\n",
    "print(\"- Utile pour indexation sparse ou √©chantillonnage al√©atoire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f914633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. COMPARAISON PRATIQUE - M√™me objectif, approches diff√©rentes\n",
      "============================================================\n",
      "OBJECTIF: S√©lectionner la 2e et 4e colonne de toutes les lignes\n",
      "\n",
      "1. Avec index_select:\n",
      "tensor([[11, 13],\n",
      "        [21, 23],\n",
      "        [31, 33],\n",
      "        [41, 43]])\n",
      "\n",
      "Indices pour gather (r√©p√©t√©s): \n",
      "tensor([[1, 3],\n",
      "        [1, 3],\n",
      "        [1, 3],\n",
      "        [1, 3]])\n",
      "2. Avec gather (√©quivalent):\n",
      "tensor([[11, 13],\n",
      "        [21, 23],\n",
      "        [31, 33],\n",
      "        [41, 43]])\n",
      "3. Avec take (plus complexe):\n",
      "tensor([[11, 13],\n",
      "        [21, 23],\n",
      "        [31, 33],\n",
      "        [41, 43]])\n",
      "\n",
      "V√©rification - tous √©gaux: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. COMPARAISON PRATIQUE - M√™me objectif, approches diff√©rentes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"OBJECTIF: S√©lectionner la 2e et 4e colonne de toutes les lignes\\n\")\n",
    "\n",
    "# M√©thode 1: avec index_select (LE PLUS SIMPLE)\n",
    "cols_wanted = torch.tensor([1, 3])\n",
    "result_index_select = torch.index_select(data, dim=1, index=cols_wanted)\n",
    "print(\"1. Avec index_select:\")\n",
    "print(result_index_select)\n",
    "\n",
    "# M√©thode 2: avec gather (PLUS VERBEUX mais plus flexible)\n",
    "# Il faut r√©p√©ter les indices pour chaque ligne\n",
    "gather_indices = cols_wanted.unsqueeze(0).expand(data.shape[0], -1)\n",
    "print(f\"\\nIndices pour gather (r√©p√©t√©s): \\n{gather_indices}\")\n",
    "result_gather = torch.gather(data, dim=1, index=gather_indices)\n",
    "print(\"2. Avec gather (√©quivalent):\")\n",
    "print(result_gather)\n",
    "\n",
    "# M√©thode 3: avec take (COMPLIQU√â)\n",
    "# Il faut calculer les positions absolutes\n",
    "take_indices = []\n",
    "for row in range(data.shape[0]):\n",
    "    for col in [1, 3]:  # colonnes 1 et 3\n",
    "        take_indices.append(row * data.shape[1] + col)\n",
    "take_indices = torch.tensor(take_indices)\n",
    "result_take = torch.take(data, take_indices).reshape(data.shape[0], 2)\n",
    "print(\"3. Avec take (plus complexe):\")\n",
    "print(result_take)\n",
    "\n",
    "print(f\"\\nV√©rification - tous √©gaux: {torch.equal(result_index_select, result_gather) and torch.equal(result_gather, result_take)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2824066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. CAS D'USAGE TYPIQUES\n",
      "============================================================\n",
      "üìå GATHER - Quand utiliser:\n",
      "‚úì Indexation diff√©rente par batch (ex: derniers √©v√©nements)\n",
      "‚úì S√©lection de √©l√©ments selon des crit√®res dynamiques\n",
      "‚úì Extraction de valeurs selon des indices calcul√©s\n",
      "‚úì Traitement par batch avec indices variables\n",
      "\n",
      "üìå INDEX_SELECT - Quand utiliser:\n",
      "‚úì S√©lection de lignes/colonnes compl√®tes\n",
      "‚úì Sous-√©chantillonnage uniforme\n",
      "‚úì R√©organisation de dimensions\n",
      "‚úì Quand tous les batches ont les m√™mes indices\n",
      "\n",
      "üìå TAKE - Quand utiliser:\n",
      "‚úì √âchantillonnage al√©atoire de positions\n",
      "‚úì Indexation sparse sur tenseurs aplatis\n",
      "‚úì Conversion d'indices 2D vers 1D\n",
      "‚úì S√©lection d'√©l√©ments non-structur√©e\n",
      "\n",
      "============================================================\n",
      "6. EXEMPLE PROCESSUS PONCTUELS\n",
      "============================================================\n",
      "√âv√©nements:\n",
      "tensor([[0, 1, 0, 0, 1],\n",
      "        [1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0]])\n",
      "Temps:\n",
      "tensor([[1.7946, 2.5892, 3.1408, 4.6366, 4.9139],\n",
      "        [1.1127, 1.3986, 3.1231, 4.1274, 5.9714],\n",
      "        [0.2518, 0.3949, 1.9774, 2.6786, 3.4462]])\n",
      "\n",
      "1. GATHER - 3e temps de chaque batch: tensor([3.1408, 3.1231, 1.9774])\n",
      "2. INDEX_SELECT - positions 2 et 4 pour tous:\n",
      "tensor([[2.5892, 4.6366],\n",
      "        [1.3986, 4.1274],\n",
      "        [0.3949, 2.6786]])\n",
      "3. TAKE - √©l√©ments aux positions absolutes [1, 7, 12]: tensor([2.5892, 3.1231, 1.9774])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. CAS D'USAGE TYPIQUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üìå GATHER - Quand utiliser:\")\n",
    "print(\"‚úì Indexation diff√©rente par batch (ex: derniers √©v√©nements)\")\n",
    "print(\"‚úì S√©lection de √©l√©ments selon des crit√®res dynamiques\")\n",
    "print(\"‚úì Extraction de valeurs selon des indices calcul√©s\")\n",
    "print(\"‚úì Traitement par batch avec indices variables\")\n",
    "\n",
    "print(\"\\nüìå INDEX_SELECT - Quand utiliser:\")\n",
    "print(\"‚úì S√©lection de lignes/colonnes compl√®tes\")\n",
    "print(\"‚úì Sous-√©chantillonnage uniforme\")\n",
    "print(\"‚úì R√©organisation de dimensions\")\n",
    "print(\"‚úì Quand tous les batches ont les m√™mes indices\")\n",
    "\n",
    "print(\"\\nüìå TAKE - Quand utiliser:\")\n",
    "print(\"‚úì √âchantillonnage al√©atoire de positions\")\n",
    "print(\"‚úì Indexation sparse sur tenseurs aplatis\")\n",
    "print(\"‚úì Conversion d'indices 2D vers 1D\")\n",
    "print(\"‚úì S√©lection d'√©l√©ments non-structur√©e\")\n",
    "\n",
    "# Exemple concret pour processus ponctuels\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. EXEMPLE PROCESSUS PONCTUELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size, seq_len, num_types = 3, 5, 2\n",
    "events = torch.randint(0, num_types, (batch_size, seq_len))\n",
    "times = torch.randn(batch_size, seq_len).abs().cumsum(dim=1)\n",
    "\n",
    "print(\"√âv√©nements:\")\n",
    "print(events)\n",
    "print(\"Temps:\")\n",
    "print(times)\n",
    "\n",
    "# Cas 1: GATHER - R√©cup√©rer le temps du 3e √©v√©nement de chaque batch\n",
    "indices_3rd = torch.tensor([[2], [2], [2]])  # 3e position pour chaque batch\n",
    "times_3rd_gather = torch.gather(times, dim=1, index=indices_3rd)\n",
    "print(f\"\\n1. GATHER - 3e temps de chaque batch: {times_3rd_gather.flatten()}\")\n",
    "\n",
    "# Cas 2: INDEX_SELECT - Prendre les 2e et 4e temps de tous les batchs\n",
    "positions = torch.tensor([1, 3])  # positions 2 et 4\n",
    "times_selected = torch.index_select(times, dim=1, index=positions)\n",
    "print(\"2. INDEX_SELECT - positions 2 et 4 pour tous:\")\n",
    "print(times_selected)\n",
    "\n",
    "# Cas 3: TAKE - R√©cup√©rer des √©l√©ments espec√≠fiques de fa√ßon sparse\n",
    "sparse_indices = torch.tensor([1, 7, 12])  # positions absolues dans le tableau plat\n",
    "times_sparse = torch.take(times, sparse_indices)\n",
    "print(f\"3. TAKE - √©l√©ments aux positions absolutes {sparse_indices.tolist()}: {times_sparse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "011d6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. TABLEAU R√âCAPITULATIF\n",
      "============================================================\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Caract√©ristique ‚îÇ    GATHER    ‚îÇ INDEX_SELECT ‚îÇ     TAKE     ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Flexibilit√©     ‚îÇ    Haute     ‚îÇ   Moyenne    ‚îÇ    Faible    ‚îÇ\n",
      "‚îÇ Complexit√©      ‚îÇ   Moyenne    ‚îÇ   Faible     ‚îÇ    Haute     ‚îÇ\n",
      "‚îÇ Shape r√©sultat  ‚îÇ = shape idx  ‚îÇ Pr√©serv√©e    ‚îÇ  Toujours 1D ‚îÇ\n",
      "‚îÇ Indices/batch   ‚îÇ Diff√©rents   ‚îÇ  Identiques  ‚îÇ   Absolus    ‚îÇ\n",
      "‚îÇ Dimensions      ‚îÇ Pr√©serv√©es   ‚îÇ Dim-1 libre  ‚îÇ  Aplaties    ‚îÇ\n",
      "‚îÇ Performance     ‚îÇ   Rapide     ‚îÇ   Rapide     ‚îÇ   Rapide     ‚îÇ\n",
      "‚îÇ Cas d'usage     ‚îÇ Batch vari√©  ‚îÇ Slicing++    ‚îÇ Sparse/1D    ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üéØ R√àGLE G√âN√âRALE:\n",
      "‚Ä¢ Utilisez INDEX_SELECT pour des s√©lections uniformes simples\n",
      "‚Ä¢ Utilisez GATHER pour des s√©lections par batch variables\n",
      "‚Ä¢ Utilisez TAKE pour des indexations 1D ou tr√®s sp√©cifiques\n",
      "\n",
      "‚ú® CONSEIL PERFORMANCE:\n",
      "‚Ä¢ Pour des s√©lections simples: slicing normal [indices] > index_select > gather\n",
      "‚Ä¢ Pour des s√©lections complexes par batch: gather est optimal\n",
      "‚Ä¢ Pour des acc√®s sparse: take peut √™tre utile mais souvent il y a mieux\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. TABLEAU R√âCAPITULATIF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Caract√©ristique ‚îÇ    GATHER    ‚îÇ INDEX_SELECT ‚îÇ     TAKE     ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Flexibilit√©     ‚îÇ    Haute     ‚îÇ   Moyenne    ‚îÇ    Faible    ‚îÇ\n",
    "‚îÇ Complexit√©      ‚îÇ   Moyenne    ‚îÇ   Faible     ‚îÇ    Haute     ‚îÇ\n",
    "‚îÇ Shape r√©sultat  ‚îÇ = shape idx  ‚îÇ Pr√©serv√©e    ‚îÇ  Toujours 1D ‚îÇ\n",
    "‚îÇ Indices/batch   ‚îÇ Diff√©rents   ‚îÇ  Identiques  ‚îÇ   Absolus    ‚îÇ\n",
    "‚îÇ Dimensions      ‚îÇ Pr√©serv√©es   ‚îÇ Dim-1 libre  ‚îÇ  Aplaties    ‚îÇ\n",
    "‚îÇ Performance     ‚îÇ   Rapide     ‚îÇ   Rapide     ‚îÇ   Rapide     ‚îÇ\n",
    "‚îÇ Cas d'usage     ‚îÇ Batch vari√©  ‚îÇ Slicing++    ‚îÇ Sparse/1D    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ R√àGLE G√âN√âRALE:\")\n",
    "print(\"‚Ä¢ Utilisez INDEX_SELECT pour des s√©lections uniformes simples\")\n",
    "print(\"‚Ä¢ Utilisez GATHER pour des s√©lections par batch variables\") \n",
    "print(\"‚Ä¢ Utilisez TAKE pour des indexations 1D ou tr√®s sp√©cifiques\")\n",
    "\n",
    "print(\"\\n‚ú® CONSEIL PERFORMANCE:\")\n",
    "print(\"‚Ä¢ Pour des s√©lections simples: slicing normal [indices] > index_select > gather\")\n",
    "print(\"‚Ä¢ Pour des s√©lections complexes par batch: gather est optimal\")\n",
    "print(\"‚Ä¢ Pour des acc√®s sparse: take peut √™tre utile mais souvent il y a mieux\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easy-tpp (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
