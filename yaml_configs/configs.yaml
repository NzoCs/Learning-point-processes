# YAML configurations for New_LTPP project
# This file contains configurations for data, models, simulations, and training (trainer)


# ===== DATA CONFIGURATIONS =====
data_configs:
  taxi:
    data_format: hf
    src_dir: easytpp/taxi
    num_event_types: 10
  
  taobao:
    data_format: hf
    src_dir: easytpp/taobao
    num_event_types: 17

  amazon:
    data_format: hf
    src_dir: easytpp/amazon
    num_event_types: 16

  retweet:
    data_format: hf
    src_dir: easytpp/retweet
    num_event_types: 3
  
  volcano:
    data_format: hf
    src_dir: easytpp/volcano
    num_event_types: 1
  
  earthquake:
    data_format: hf
    src_dir: easytpp/earthquake
    num_event_types: 7
  
  stackoverflow:
    data_format: hf
    src_dir: easytpp/stackoverflow
    num_event_types: 22

  test:
    data_format: hf
    src_dir: NzoCs/test_dataset
    num_event_types: 2
      
  hawkes1:
    data_format: hf
    src_dir: NzoCs/hawkes1
    num_event_types: 1

  hawkes2:
    data_format: hf
    src_dir: NzoCs/hawkes2
    num_event_types: 2
  
  H2expi:
    data_format: hf
    src_dir: NzoCs/H2expi
    num_event_types: 2
  
  H2expc:
    data_format: hf
    src_dir: NzoCs/H2expc
    num_event_types: 2

# ==== LOGGER CONFIGS =====
logger_configs:

  tensorboard:
    type: tensorboard
    save_dir: logs/tensorboard_logs
    config:
      name: tb_logs
      flush_secs: 120
      version: 1

  csv:
    type: csv
    save_dir: logs/csv_logs
    config:
      name: csv_logs
      version: 1

  wandb:
    type: wandb
    save_dir: logs/wandb_logs
    config:
      entity: your_wandb_entity
      project: new_ltpp_project
      name: wandb_logs
      notes: "Experiment run"
      tags: [new_ltpp, experiment]
      resume: false
      anonymous: false

  # mlflow: # Not working yet
  #   type: mlflow
  #   save_dir: logs/mlflow
  #   config:
  #     experiment_name: new_ltpp_experiments
  #     tracking_uri: http://localhost:5000
  #     run_name: run_1

  # comet: # Not tested yet
  #   type: comet
  #   save_dir: logs/comet
  #   config:
  #     api_key: "YOUR_COMET_API_KEY"
  #     project_name: new_ltpp_project
  #     workspace: your_workspace

  # neptune: # Not tested yet
  #   type: neptune
  #   save_dir: logs/neptune
  #   config:
  #     api_token: "YOUR_NEPTUNE_API_TOKEN"
  #     project: your_neptune_project
  #     run_name: run_1

# ===== MODEL CONFIGURATIONS =====
model_configs:
# Configurations for model-specific parameters (architecture, hyperparameters, etc.)

  quick_test:
    specs:
      hidden_size: 8
      time_emb_size: 4
      num_layers: 1
      num_heads: 1  # For attention-based models
  
  debug:
    specs:
      hidden_size: 8
      time_emb_size: 4
      num_layers: 1
      num_heads: 1  # For attention-based models

  # Configurations for neural models - Size Small
  h16_e8_l1_h2:
    specs:
      hidden_size: 16
      time_emb_size: 8
      num_layers: 1
      num_heads: 2  # For attention-based models

  # Configurations for neural models - Size Medium
  h32_e16_l2_h4:
    specs:
      hidden_size: 32
      time_emb_size: 16
      num_layers: 2
      num_heads: 4  # For attention-based models

  # Configurations for neural models - Size Large
  h64_e32_l3_h8:
    specs:
      hidden_size: 64
      time_emb_size: 32
      num_layers: 3
      num_heads: 8  # For attention-based models

  # Configurations for neural models - Size Extra Large
  h128_e64_l4_h16:
    specs:
      hidden_size: 128
      time_emb_size: 64
      num_layers: 4
      num_heads: 16  # For attention-based models

  # Specific configurations for Hawkes - Simple (univariate)
  hawkes1:
    specs:
      mu: 0.2
      alpha: 0.8
      beta: 1.0

  # Specific configurations for Hawkes - Bivariate
  hawkes2:
    specs:
      mu: [0.2, 0.2]
      alpha: [[0.4, 0], [0, 0.8]]
      beta: [[1.0, 0], [0, 1.0]]

  # Specific configurations for Hawkes - Complex multivariate
  hawkes_multivariate:
    specs:
      mu: [0.2, 0.2, 0.1]
      alpha: [[0.4, 0.1, 0], [0.1, 0.8, 0.2], [0, 0.2, 0.6]]
      beta: [[1.0, 0.5, 0], [0.5, 1.0, 0.8], [0, 0.8, 1.0]]


# ===== THINNING CONFIGURATIONS =====
thinning_configs:

  quick_test:
    num_exp: 10
    over_sample_rate: 1.1
    num_sample: 5
  
  debug:
    num_exp: 10
    over_sample_rate: 1.1
    num_sample: 5

  # Fast thinning configuration (for tests)
  e50_s15:
    num_exp: 50
    over_sample_rate: 1.2
    num_sample: 15

  # Standard thinning configuration
  e100_s30:
    num_exp: 100
    over_sample_rate: 1.5
    num_sample: 30

  # High-quality thinning configuration
  e150_s50:
    num_exp: 150
    over_sample_rate: 1.8
    num_sample: 50

  # Thinning configuration for large datasets
  e200_s60:
    num_exp: 200
    over_sample_rate: 2.0
    num_sample: 60

  # Minimal thinning configuration (debug/CPU)
  e20_s10:
    num_exp: 20
    over_sample_rate: 1.1
    num_sample: 10


# ===== SIMULATION CONFIGURATIONS =====
simulation_configs:

  quick_test:
    start_time: 0
    end_time: 120
    max_sim_events: 1000
    batch_size: 4
  
  debug:
    start_time: 0
    end_time: 20
    max_sim_events: 2000
    batch_size: 8

  # Fast simulation configuration (tests)
  s20_e50_5000_b16:
    start_time: 20
    end_time: 50
    max_sim_events: 5000
    batch_size: 16

  # Development simulation configuration
  s50_e120_15000_b32:
    start_time: 50
    end_time: 120
    max_sim_events: 15000
    batch_size: 32

  # Standard simulation configuration
  s100_e200_50000_b64:
    start_time: 100
    end_time: 200
    max_sim_events: 50000
    batch_size: 64

  # Extended simulation configuration
  s100_e300_100000_b128:
    start_time: 100
    end_time: 300
    max_sim_events: 100000
    batch_size: 128

  # Simulation configuration for large datasets
  s100_e400_150000_b256:
    start_time: 100
    end_time: 400
    max_sim_events: 150000
    batch_size: 256

  # Simulation configuration for cross-validation
  s80_e160_30000_b64:
    start_time: 80
    end_time: 160
    max_sim_events: 30000
    batch_size: 64

  # Simulation configuration for experiments
  s50_e120_15000_b32:
    start_time: 50
    end_time: 120
    max_sim_events: 15000
    batch_size: 32

# ===== DATA LOADING CONFIGURATIONS =====
# These configurations contain only DataLoader parameters

data_loading_configs:
  
  debug:
    batch_size: 1
    num_workers: 1

  quick_test:
    batch_size: 8
    num_workers: 1

  b32_w1:
    batch_size: 32
    num_workers: 1

  b64_w2:
    batch_size: 64
    num_workers: 2

  b128_w4:
    batch_size: 128
    num_workers: 4

  b64_w4:
    batch_size: 64
    num_workers: 4

  b32_w2:
    batch_size: 32
    num_workers: 2

  b16_w1:
    batch_size: 256
    num_workers: 8

  b128_w6:
    batch_size: 128
    num_workers: 6

  b512_w8:
    batch_size: 512
    num_workers: 8


# ===== TRAINING CONFIGURATIONS (TRAINER) =====
# These configurations contain only training parameters
# Data, model and simulation configurations are defined separately

training_configs:
  # Quick test configuration
  quick_test:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3

  # Debug configuration
  debug:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3

  # Standard training configuration
  e500_b1:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 20

  # Standard configuration with very small batch (very limited memory)
  e500_b4:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the very small batch
    patience: 20

  # Production configuration with medium batch (limited memory)
  e1000_b2:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 2  # Compensate for the smaller batch
    patience: 50

  # Production configuration with small batch (very limited memory)
  e1000_b4:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the small batch
    patience: 50
