# YAML configurations for New_LTPP project
# This file contains configurations for data, models, simulations, and training (trainer)


# ===== DATA CONFIGURATIONS =====
data_configs:
  taxi:
    data_format: json
    src_dir: new_ltpp/taxi
    num_event_types: 10
  
  taobao:
    data_format: json
    src_dir: new_ltpp/taobao
    num_event_types: 17

  amazon:
    data_format: json
    src_dir: new_ltpp/amazon
    num_event_types: 16

  test:
    data_format: json
    src_dir: NzoCs/test_dataset
    num_event_types: 2
      
  hawkes1:
    data_format: json
    src_dir: NzoCs/hawkes1
    num_event_types: 1

  hawkes2:
    data_format: json
    src_dir: NzoCs/hawkes2
    num_event_types: 2
  
  H2expi:
    data_format: json
    src_dir: NzoCs/H2expi
    num_event_types: 2
  
  H2expc:
    data_format: json
    src_dir: NzoCs/H2expc
    num_event_types: 2

# ==== LOGGER CONFIGS =====
logger_configs:
  tensorboard:
    type: tensorboard
    save_dir: logs/tensorboard_logs
    config:
      name: tb_logs
      flush_secs: 120
      version: 1

  csv:
    type: csv
    save_dir: logs/csv_logs
    config:
      name: csv_logs
      version: 1

  wandb:
    type: wandb
    save_dir: logs/wandb_logs
    config:
      entity: your_wandb_entity
      project: new_ltpp_project
      name: wandb_logs
      notes: "Experiment run"
      tags: [new_ltpp, experiment]
      resume: false
      anonymous: false

  mlflow: # Not working yet
    type: mlflow
    save_dir: logs/mlflow
    config:
      experiment_name: new_ltpp_experiments
      tracking_uri: http://localhost:5000
      run_name: run_1

  comet: # Not tested yet
    type: comet
    save_dir: logs/comet
    config:
      api_key: "YOUR_COMET_API_KEY"
      project_name: new_ltpp_project
      workspace: your_workspace

  neptune: # Not tested yet
    type: neptune
    save_dir: logs/neptune
    config:
      api_token: "YOUR_NEPTUNE_API_TOKEN"
      project: your_neptune_project
      run_name: run_1

# ===== MODEL CONFIGURATIONS =====
model_configs:
# Configurations for model-specific parameters (architecture, hyperparameters, etc.)

  # Configurations for neural models - Size Small
  neural_small:
    specs:
      hidden_size: 16
      time_emb_size: 8
      num_layers: 1
      num_heads: 2  # For attention-based models

  # Configurations for neural models - Size Medium
  neural_medium:
    specs:
      hidden_size: 32
      time_emb_size: 16
      num_layers: 2
      num_heads: 4  # For attention-based models

  # Configurations for neural models - Size Large
  neural_large:
    specs:
      hidden_size: 64
      time_emb_size: 32
      num_layers: 3
      num_heads: 8  # For attention-based models

  # Configurations for neural models - Size Extra Large
  neural_xlarge:
    specs:
      hidden_size: 128
      time_emb_size: 64
      num_layers: 4
      num_heads: 16  # For attention-based models

  # Specific configurations for Hawkes - Simple (univariate)
  hawkes_simple:
    specs:
      mu: 0.2
      alpha: 0.8
      beta: 1.0

  # Specific configurations for Hawkes - Bivariate
  hawkes_bivariate:
    specs:
      mu: [0.2, 0.2]
      alpha: [[0.4, 0], [0, 0.8]]
      beta: [[1.0, 0], [0, 1.0]]

  # Specific configurations for Hawkes - Complex multivariate
  hawkes_multivariate:
    specs:
      mu: [0.2, 0.2, 0.1]
      alpha: [[0.4, 0.1, 0], [0.1, 0.8, 0.2], [0, 0.2, 0.6]]
      beta: [[1.0, 0.5, 0], [0.5, 1.0, 0.8], [0, 0.8, 1.0]]


# ===== THINNING CONFIGURATIONS =====
thinning_configs:

  # Fast thinning configuration (for tests)
  thinning_fast:
    num_exp: 50
    over_sample_rate: 1.2
    dtime_max: 3
    num_sample: 15

  # Standard thinning configuration
  thinning_standard:
    num_exp: 100
    over_sample_rate: 1.5
    dtime_max: 5
    num_sample: 30

  # High-quality thinning configuration
  thinning_high_quality:
    num_exp: 150
    over_sample_rate: 1.8
    dtime_max: 8
    num_sample: 50

  # Thinning configuration for large datasets
  thinning_large_dataset:
    num_exp: 200
    over_sample_rate: 2.0
    dtime_max: 10
    num_sample: 60

  # Thinning configuration for FullyNN (more samples)
  thinning_fullyNN:
    num_exp: 200
    over_sample_rate: 1.5
    dtime_max: 5
    num_sample: 60

  # Minimal thinning configuration (debug/CPU)
  thinning_minimal:
    num_exp: 20
    over_sample_rate: 1.1
    dtime_max: 2
    num_sample: 10


# ===== SIMULATION CONFIGURATIONS =====
simulation_configs:

  # Fast simulation configuration (tests)
  simulation_fast:
    start_time: 20
    end_time: 50
    max_sim_events: 5000
    batch_size: 16

  # Development simulation configuration
  simulation_dev:
    start_time: 50
    end_time: 120
    max_sim_events: 15000
    batch_size: 32

  # Standard simulation configuration
  simulation_standard:
    start_time: 100
    end_time: 200
    max_sim_events: 50000
    batch_size: 64

  # Extended simulation configuration
  simulation_extended:
    start_time: 100
    end_time: 300
    max_sim_events: 100000
    batch_size: 128

  # Simulation configuration for large datasets
  simulation_large:
    start_time: 100
    end_time: 400
    max_sim_events: 150000
    batch_size: 256

  # Simulation configuration for cross-validation
  simulation_cv:
    start_time: 80
    end_time: 160
    max_sim_events: 30000
    batch_size: 64

  # Simulation configuration for experiments
  simulation_experiment:
    start_time: 50
    end_time: 120
    max_sim_events: 15000
    batch_size: 32

# ===== DATA LOADING CONFIGURATIONS =====
# These configurations contain only DataLoader parameters

data_loading_configs:

  quick_test:
    batch_size: 32
    num_workers: 1

  dev:
    batch_size: 64
    num_workers: 2

  standard:
    batch_size: 128
    num_workers: 4

  standard_small_batch:
    batch_size: 64
    num_workers: 4

  standard_tiny_batch:
    batch_size: 32
    num_workers: 2

  production:
    batch_size: 256
    num_workers: 8

  production_medium_batch:
    batch_size: 128
    num_workers: 6

  production_small_batch:
    batch_size: 64
    num_workers: 4

  production_tiny_batch:
    batch_size: 32
    num_workers: 2

  production_micro_batch:
    batch_size: 16
    num_workers: 1

  large_dataset:
    batch_size: 512
    num_workers: 8

  debug:
    batch_size: 8
    num_workers: 1

  experiment:
    batch_size: 16
    num_workers: 1

  cross_validation:
    batch_size: 64
    num_workers: 4

  hpo:
    batch_size: 32
    num_workers: 2

  intensive:
    batch_size: 1024
    num_workers: 12

# ===== TRAINING CONFIGURATIONS (TRAINER) =====
# These configurations contain only training parameters
# Data, model and simulation configurations are defined separately

training_configs:
  # Quick test configuration
  quick_test:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3

  # Development configuration
  dev:
    max_epochs: 50
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 10

  # Standard training configuration
  standard:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 20

  # Standard configuration with smaller batch (limited memory)
  standard_small_batch:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 2  # Compensate for the small batch
    patience: 20

  # Standard configuration with very small batch (very limited memory)
  standard_tiny_batch:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the very small batch
    patience: 20

  # Long training configuration
  production:
      max_epochs: 1000
      val_freq: 10
      accumulate_grad_batches: 1
      patience: 50

  # Production configuration with medium batch (limited memory)
  production_medium_batch:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 2  # Compensate for the smaller batch
    patience: 50

  # Production configuration with small batch (very limited memory)
  production_small_batch:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the small batch
    patience: 50

  # Production configuration with very small batch (low-GPU)
  production_tiny_batch:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 8  # Compensate for the very small batch
    patience: 50

  # Production configuration with micro batch (very weak CPU or GPU)
  production_micro_batch:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 16  # Compensate for the micro batch
    patience: 50

  # Configuration for large datasets (larger batch)
  large_dataset:
    max_epochs: 200
    val_freq: 5
    accumulate_grad_batches: 2
    patience: 30

  # Debug configuration (very small)
  debug:
    max_epochs: 3
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 2

  # Quick experiment configuration
  experiment:
    max_epochs: 10
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 5

  # Cross-validation configuration
  cross_validation:
    max_epochs: 100
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 15

  # Hyperparameter tuning configuration
  hpo:
    max_epochs: 20
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 8

  # Intensive training configuration (powerful GPU)
  intensive:
    max_epochs: 2000
    val_freq: 20
    accumulate_grad_batches: 4
    patience: 100
