# YAML configurations for New_LTPP project
# This file contains configurations for data, models, simulations, and training (trainer)


# ===== DATA CONFIGURATIONS =====
data_configs:
  taxi:
    data_format: hf
    src_dir: easytpp/taxi
    num_event_types: 10
  
  taobao:
    data_format: hf
    src_dir: easytpp/taobao
    num_event_types: 17

  amazon:
    data_format: hf
    src_dir: easytpp/amazon
    num_event_types: 16

  retweet:
    data_format: hf
    src_dir: easytpp/retweet
    num_event_types: 3
  
  volcano:
    data_format: hf
    src_dir: easytpp/volcano
    num_event_types: 1
  
  earthquake:
    data_format: hf
    src_dir: easytpp/earthquake
    num_event_types: 7
  
  stackoverflow:
    data_format: hf
    src_dir: easytpp/stackoverflow
    num_event_types: 22

  test:
    data_format: hf
    src_dir: NzoCs/test_dataset
    num_event_types: 2
      
  hawkes1:
    data_format: hf
    src_dir: NzoCs/hawkes1
    num_event_types: 1

  hawkes2:
    data_format: hf
    src_dir: NzoCs/hawkes2
    num_event_types: 2
  
  H2expi:
    data_format: hf
    src_dir: NzoCs/H2expi
    num_event_types: 2
  
  H2expc:
    data_format: hf
    src_dir: NzoCs/H2expc
    num_event_types: 2

# ==== LOGGER CONFIGS =====
logger_configs:

  tensorboard:
    type: tensorboard
    save_dir: logs/tensorboard_logs
    config:
      name: tb_logs
      flush_secs: 120
      version: 1

  csv:
    type: csv
    save_dir: logs/csv_logs
    config:
      name: csv_logs
      version: 1

  wandb:
    type: wandb
    save_dir: logs/wandb_logs
    config:
      entity: your_wandb_entity
      project: new_ltpp_project
      name: wandb_logs
      notes: "Experiment run"
      tags: [new_ltpp, experiment]
      resume: false
      anonymous: false

  # mlflow: # Not working yet
  #   type: mlflow
  #   save_dir: logs/mlflow
  #   config:
  #     experiment_name: new_ltpp_experiments
  #     tracking_uri: http://localhost:5000
  #     run_name: run_1

  # comet: # Not tested yet
  #   type: comet
  #   save_dir: logs/comet
  #   config:
  #     api_key: "YOUR_COMET_API_KEY"
  #     project_name: new_ltpp_project
  #     workspace: your_workspace

  # neptune: # Not tested yet
  #   type: neptune
  #   save_dir: logs/neptune
  #   config:
  #     api_token: "YOUR_NEPTUNE_API_TOKEN"
  #     project: your_neptune_project
  #     run_name: run_1

# ===== MODEL CONFIGURATIONS =====
model_configs:
# Configurations for model-specific parameters (architecture, hyperparameters, etc.)

  quick_test:
    specs:
      hidden_size: 8
  
  debug:
    specs:
      hidden_size: 8

  # Configurations for neural models - Size Small
  h16_e8_l1_h2:
    specs:
      hidden_size: 16

  # Configurations for neural models - Size Medium
  h32_e16_l2_h4:
    specs:
      hidden_size: 32

  # Configurations for neural models - Size Large
  h64_e32_l3_h8:
    specs:
      hidden_size: 64

  # Configurations for neural models - Size Extra Large
  h128_e64_l4_h16:
    specs:
      hidden_size: 128


# ===== THINNING CONFIGURATIONS =====
thinning_configs:

  quick_test:
    num_exp: 10
    over_sample_rate: 1.1
    num_sample: 5
  
  debug:
    num_exp: 10
    over_sample_rate: 1.1
    num_sample: 5

  # Fast thinning configuration (for tests)
  e50_s15:
    num_exp: 50
    over_sample_rate: 1.2
    num_sample: 15

  # Standard thinning configuration
  e100_s30:
    num_exp: 100
    over_sample_rate: 1.5
    num_sample: 30

  # High-quality thinning configuration
  e150_s50:
    num_exp: 150
    over_sample_rate: 1.8
    num_sample: 50

  # Thinning configuration for large datasets
  e200_s60:
    num_exp: 200
    over_sample_rate: 2.0
    num_sample: 60

  # Minimal thinning configuration (debug/CPU)
  e20_s10:
    num_exp: 20
    over_sample_rate: 1.1
    num_sample: 10


# ===== SIMULATION CONFIGURATIONS =====
simulation_configs:

  quick_test:
    time_window: 120
    initial_buffer_size: 1000
    batch_size: 4
  
  debug:
    time_window: 20
    initial_buffer_size: 2000
    batch_size: 8

  # Fast simulation configuration (tests)
  tw30_5000_b16:
    time_window: 30
    initial_buffer_size: 5000
    batch_size: 16

  # Development simulation configuration
  tw70_15000_b32:
    time_window: 70
    initial_buffer_size: 15000
    batch_size: 32

  # Standard simulation configuration
  tw100_50000_b64:
    time_window: 100
    initial_buffer_size: 50000
    batch_size: 64

  # Extended simulation configuration
  tw200_100000_b128:
    time_window: 200
    initial_buffer_size: 100000
    batch_size: 128

  # Simulation configuration for large datasets
  tw300_150000_b256:
    time_window: 300
    initial_buffer_size: 150000
    batch_size: 256

  # Simulation configuration for cross-validation
  tw80_30000_b64:
    time_window: 80
    initial_buffer_size: 30000
    batch_size: 64

  # Simulation configuration for experiments
  tw70_15000_b32:
    time_window: 70
    initial_buffer_size: 15000
    batch_size: 32

# ===== DATA LOADING CONFIGURATIONS =====
# These configurations contain only DataLoader parameters

data_loading_configs:
  
  debug:
    batch_size: 1
    num_workers: 1

  quick_test:
    batch_size: 8
    num_workers: 1

  b32_w1:
    batch_size: 32
    num_workers: 1

  b64_w2:
    batch_size: 64
    num_workers: 2

  b128_w4:
    batch_size: 128
    num_workers: 4

  b64_w4:
    batch_size: 64
    num_workers: 4

  b32_w2:
    batch_size: 32
    num_workers: 2

  b16_w1:
    batch_size: 256
    num_workers: 8

  b128_w6:
    batch_size: 128
    num_workers: 6

  b512_w8:
    batch_size: 512
    num_workers: 8


# ===== TRAINING CONFIGURATIONS (TRAINER) =====
# These configurations contain only training parameters
# Data, model and simulation configurations are defined separately

training_configs:
  # Quick test configuration
  quick_test:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3

  # Debug configuration
  debug:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3

  # Standard training configuration
  e500_b1:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 20

  # Standard configuration with very small batch (very limited memory)
  e500_b4:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the very small batch
    patience: 20

  # Production configuration with medium batch (limited memory)
  e1000_b2:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 2  # Compensate for the smaller batch
    patience: 50

  # Production configuration with small batch (very limited memory)
  e1000_b4:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 4  # Compensate for the small batch
    patience: 50
