# ===== TRAINING CONFIGURATIONS (TRAINER) =====
# These configurations contain only training parameters
# Data, model and simulation configurations are defined separately

# Quick test configuration
quick_test:
  data_loading_specs:
    batch_size: 32
    num_workers: 1
    
  training_config:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3
    logger_config:
      type: tensorboard
      name: quick_test_logs

# Development configuration
dev:
  data_loading_specs:
    batch_size: 64
    num_workers: 2
    
  training_config:
    max_epochs: 50
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 10
    logger_config:
      type: tensorboard
      name: dev_logs

# Standard training configuration
standard:
  data_loading_specs:
    batch_size: 128
    num_workers: 4
    
  training_config:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_logs

# Standard configuration with smaller batch (limited memory)
standard_small_batch:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 500
    val_freq: 10
  accumulate_grad_batches: 2  # Compensate for the small batch
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_small_batch_logs

# Standard configuration with very small batch (very limited memory)
standard_tiny_batch:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 500
    val_freq: 10
  accumulate_grad_batches: 4  # Compensate for the very small batch
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_tiny_batch_logs

# Long training configuration
production:
  data_loading_specs:
    batch_size: 256
    num_workers: 8
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 50
    logger_config:
      type: tensorboard
      name: production_logs

# Production configuration with medium batch (limited memory)
production_medium_batch:
  data_loading_specs:
    batch_size: 128
    num_workers: 6
    
  training_config:
    max_epochs: 1000
    val_freq: 10
  accumulate_grad_batches: 2  # Compensate for the smaller batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_medium_batch_logs

# Production configuration with small batch (very limited memory)
production_small_batch:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 1000
    val_freq: 10
  accumulate_grad_batches: 4  # Compensate for the small batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_small_batch_logs

# Production configuration with very small batch (low-GPU)
production_tiny_batch:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 1000
    val_freq: 10
  accumulate_grad_batches: 8  # Compensate for the very small batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_tiny_batch_logs

# Production configuration with micro batch (very weak CPU or GPU)
production_micro_batch:
  data_loading_specs:
    batch_size: 16
    num_workers: 1
    
  training_config:
    max_epochs: 1000
    val_freq: 10
  accumulate_grad_batches: 16  # Compensate for the micro batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_micro_batch_logs

# Configuration for large datasets (larger batch)
large_dataset:
  data_loading_specs:
    batch_size: 512
    num_workers: 8
    
  training_config:
    max_epochs: 200
    val_freq: 5
    accumulate_grad_batches: 2
    patience: 30
    logger_config:
      type: tensorboard
      name: large_dataset_logs

# Debug configuration (very small)
debug:
  data_loading_specs:
    batch_size: 8
    num_workers: 1
    
  training_config:
    max_epochs: 3
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 2
    logger_config:
      type: tensorboard
      name: debug_logs

# Quick experiment configuration
experiment:
  data_loading_specs:
    batch_size: 16
    num_workers: 1
    
  training_config:
    max_epochs: 10
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 5
    logger_config:
      type: tensorboard
      name: experiment_logs

# Cross-validation configuration
cross_validation:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 100
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 15
    logger_config:
      type: tensorboard
      name: cv_logs

# Hyperparameter tuning configuration
hpo:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 20
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 8
    logger_config:
      type: tensorboard
      name: hpo_logs

# Intensive training configuration (powerful GPU)
intensive:
  data_loading_specs:
    batch_size: 1024
    num_workers: 12
    
  training_config:
    max_epochs: 2000
    val_freq: 20
    accumulate_grad_batches: 4
    patience: 100
    logger_config:
      type: tensorboard
      name: intensive_logs

# ===== USAGE EXAMPLES =====
#
# These trainer configurations contain only training parameters:
# - data_loading_specs: batch size, number of workers
# - training_config: epochs, validation frequency, patience, logging
#
# Data, model, and simulation configurations are defined separately.
#
# Configurations optimized for different memory constraints:
#
# üî• Powerful GPU (12GB+ VRAM):
# - intensive: batch=1024, epochs=2000
# - large_dataset: batch=512, epochs=200  
# - production: batch=256, epochs=1000
#
# üí™ Mid-range GPU (6-8GB VRAM):
# - production_medium_batch: batch=128, epochs=1000
# - standard: batch=128, epochs=500
#
# ü§è Low GPU (4GB VRAM):
# - production_small_batch: batch=64, epochs=1000
# - standard_small_batch: batch=64, epochs=500
#
# üê£ Very low GPU (2GB VRAM) or CPU:
# - production_tiny_batch: batch=32, epochs=1000
# - production_micro_batch: batch=16, epochs=1000
# - standard_tiny_batch: batch=32, epochs=500
#
# ‚ö° Testing & Experimentation:
# - debug: batch=8, epochs=3
# - experiment: batch=16, epochs=10
# - quick_test: batch=32, epochs=5
# - dev: batch=64, epochs=50
#
# üîç Research & Validation:
# - hpo: batch=32, epochs=20
# - cross_validation: batch=64, epochs=100
#
# Benefits:
# - ‚úÖ Automatic gradient accumulation to compensate small batches
# - ‚úÖ Adjust num_workers according to available capacity
# - ‚úÖ Same training quality regardless of batch_size
# - ‚úÖ Specialized configurations for each memory tier
# - ‚úÖ Enables training on a wide range of hardware