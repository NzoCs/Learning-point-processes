# ===== CONFIGURATIONS D'ENTRA√éNEMENT (TRAINER) =====
# Ces configurations contiennent uniquement les param√®tres d'entra√Ænement
# Les donn√©es, mod√®les et simulations sont configur√©s s√©par√©ment

# Configuration pour tests rapides
quick_test:
  data_loading_specs:
    batch_size: 32
    num_workers: 1
    
  training_config:
    max_epochs: 5
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 3
    logger_config:
      type: tensorboard
      name: quick_test_logs

# Configuration pour d√©veloppement
dev:
  data_loading_specs:
    batch_size: 64
    num_workers: 2
    
  training_config:
    max_epochs: 50
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 10
    logger_config:
      type: tensorboard
      name: dev_logs

# Configuration pour entra√Ænement standard
standard:
  data_loading_specs:
    batch_size: 128
    num_workers: 4
    
  training_config:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_logs

# Configuration standard avec batch plus petit (m√©moire limit√©e)
standard_small_batch:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 2  # Compense le petit batch
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_small_batch_logs

# Configuration standard avec tr√®s petit batch (m√©moire tr√®s limit√©e)
standard_tiny_batch:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 500
    val_freq: 10
    accumulate_grad_batches: 4  # Compense le tr√®s petit batch
    patience: 20
    logger_config:
      type: tensorboard
      name: standard_tiny_batch_logs

# Configuration pour entra√Ænement long
production:
  data_loading_specs:
    batch_size: 256
    num_workers: 8
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 1
    patience: 50
    logger_config:
      type: tensorboard
      name: production_logs

# Configuration production avec batch moyen (m√©moire limit√©e)
production_medium_batch:
  data_loading_specs:
    batch_size: 128
    num_workers: 6
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 2  # Compense le batch plus petit
    patience: 50
    logger_config:
      type: tensorboard
      name: production_medium_batch_logs

# Configuration production avec petit batch (m√©moire tr√®s limit√©e)
production_small_batch:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 4  # Compense le petit batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_small_batch_logs

# Configuration production avec tr√®s petit batch (GPU faible)
production_tiny_batch:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 8  # Compense le tr√®s petit batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_tiny_batch_logs

# Configuration production avec micro batch (CPU ou GPU tr√®s faible)
production_micro_batch:
  data_loading_specs:
    batch_size: 16
    num_workers: 1
    
  training_config:
    max_epochs: 1000
    val_freq: 10
    accumulate_grad_batches: 16  # Compense le micro batch
    patience: 50
    logger_config:
      type: tensorboard
      name: production_micro_batch_logs

# Configuration pour gros datasets (batch plus large)
large_dataset:
  data_loading_specs:
    batch_size: 512
    num_workers: 8
    
  training_config:
    max_epochs: 200
    val_freq: 5
    accumulate_grad_batches: 2
    patience: 30
    logger_config:
      type: tensorboard
      name: large_dataset_logs

# Configuration pour debugging (tr√®s petit)
debug:
  data_loading_specs:
    batch_size: 8
    num_workers: 1
    
  training_config:
    max_epochs: 3
    val_freq: 1
    accumulate_grad_batches: 1
    patience: 2
    logger_config:
      type: tensorboard
      name: debug_logs

# Configuration pour exp√©rimentation rapide
experiment:
  data_loading_specs:
    batch_size: 16
    num_workers: 1
    
  training_config:
    max_epochs: 10
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 5
    logger_config:
      type: tensorboard
      name: experiment_logs

# Configuration pour validation crois√©e
cross_validation:
  data_loading_specs:
    batch_size: 64
    num_workers: 4
    
  training_config:
    max_epochs: 100
    val_freq: 5
    accumulate_grad_batches: 1
    patience: 15
    logger_config:
      type: tensorboard
      name: cv_logs

# Configuration pour hyperparameter tuning
hpo:
  data_loading_specs:
    batch_size: 32
    num_workers: 2
    
  training_config:
    max_epochs: 20
    val_freq: 2
    accumulate_grad_batches: 1
    patience: 8
    logger_config:
      type: tensorboard
      name: hpo_logs

# Configuration pour entra√Ænement intensif (GPU puissant)
intensive:
  data_loading_specs:
    batch_size: 1024
    num_workers: 12
    
  training_config:
    max_epochs: 2000
    val_freq: 20
    accumulate_grad_batches: 4
    patience: 100
    logger_config:
      type: tensorboard
      name: intensive_logs

# ===== EXEMPLES D'USAGE =====
# 
# Ces configurations de trainer contiennent uniquement les param√®tres d'entra√Ænement :
# - data_loading_specs : taille de batch, nombre de workers
# - training_config : epochs, validation, patience, logging
#
# Les autres aspects (donn√©es, mod√®les, simulations) sont configur√©s s√©par√©ment.
#
# Configurations optimis√©es selon les contraintes de m√©moire :
#
# üî• GPU Puissant (12GB+ VRAM) :
# - intensive : batch=1024, epochs=2000
# - large_dataset : batch=512, epochs=200  
# - production : batch=256, epochs=1000
#
# üí™ GPU Moyen (6-8GB VRAM) :
# - production_medium_batch : batch=128, epochs=1000
# - standard : batch=128, epochs=500
#
# ü§è GPU Faible (4GB VRAM) :
# - production_small_batch : batch=64, epochs=1000
# - standard_small_batch : batch=64, epochs=500
#
# üê£ GPU Tr√®s Faible (2GB VRAM) ou CPU :
# - production_tiny_batch : batch=32, epochs=1000
# - production_micro_batch : batch=16, epochs=1000
# - standard_tiny_batch : batch=32, epochs=500
#
# ‚ö° Test & Exp√©rimentation :
# - debug : batch=8, epochs=3
# - experiment : batch=16, epochs=10
# - quick_test : batch=32, epochs=5
# - dev : batch=64, epochs=50
#
# üîç Recherche & Validation :
# - hpo : batch=32, epochs=20
# - cross_validation : batch=64, epochs=100
#
# Avantages :
# - ‚úÖ Gradient accumulation automatique pour compenser les petits batches
# - ‚úÖ Ajustement des num_workers selon la capacit√© disponible
# - ‚úÖ M√™me qualit√© d'entra√Ænement ind√©pendamment du batch_size
# - ‚úÖ Configurations sp√©cialis√©es pour chaque niveau de m√©moire
# - ‚úÖ Permet d'entra√Æner sur n'importe quel mat√©riel